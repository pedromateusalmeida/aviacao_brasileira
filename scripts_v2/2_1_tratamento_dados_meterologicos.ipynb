{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf317e5f-b68a-4845-bf73-295fb0ff4568",
   "metadata": {},
   "source": [
    "# Pacote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f51914b-758a-482b-8088-5f88866a8633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import chardet\n",
    "import glob\n",
    "from unidecode import unidecode\n",
    "import os\n",
    "import re\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27528f5b-f7b7-44fb-afbf-a131e368a621",
   "metadata": {},
   "source": [
    "# Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b33d989-72d4-4e6c-95a2-8b729552fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    \"\"\"\n",
    "    Carrega e transforma dados meteorológicos de um arquivo CSV em uma estrutura de DataFrame requerida.\n",
    "    \n",
    "    O arquivo CSV é processado para incluir informações da estação e converter coordenadas e altitude para float.\n",
    "\n",
    "    Parâmetros:\n",
    "        file_path (str): Caminho completo para o arquivo CSV a ser processado.\n",
    "    \n",
    "    Retorna:\n",
    "        DataFrame: Um DataFrame do pandas com os dados da estação meteorológica processados.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Carregar os dados pulando as primeiras 8 linhas que contêm informações da estação\n",
    "    df = pd.read_csv(file_path, delimiter=';', encoding='ISO-8859-1', skiprows=8)\n",
    "    \n",
    "    # Carregar as informações da estação\n",
    "    station_info_df = pd.read_csv(file_path, delimiter=';', encoding='ISO-8859-1', nrows=8, header=None)\n",
    "    station_info_df.columns = ['Variable', 'Value']\n",
    "    station_info_dict = station_info_df.set_index('Variable')['Value'].to_dict()\n",
    "\n",
    "     # Converter latitude, longitude e altitude para float, substituindo vírgula por ponto\n",
    "    for key in ['LATITUDE:', 'LONGITUDE:', 'ALTITUDE:']:\n",
    "        if key in station_info_dict and isinstance(station_info_dict[key], str):\n",
    "            try:\n",
    "                # Tentativa de converter o valor para float após substituir vírgula por ponto\n",
    "                station_info_dict[key] = float(station_info_dict[key].replace(',', '.'))\n",
    "            except ValueError:\n",
    "                # Tratar o erro de conversão, por exemplo, definindo o valor como None ou um valor padrão\n",
    "                station_info_dict[key] = None  # ou algum valor padrão, se apropriado\n",
    "\n",
    "    # Adicionar informações da estação como novas colunas ao DataFrame\n",
    "    for variable, value in station_info_dict.items():\n",
    "        # Removendo os dois pontos do final do nome da variável, se houver\n",
    "        df[variable.rstrip(':')] = value\n",
    "    \n",
    "    return df\n",
    "\n",
    "def detect_encoding(file_pattern_or_path, num_bytes=10000):\n",
    "    \"\"\"\n",
    "    Detecta a codificação do arquivo ou arquivos fornecidos.\n",
    "    \n",
    "    Parâmetros:\n",
    "        file_pattern_or_path (str): Caminho ou padrão do arquivo para detecção.\n",
    "        num_bytes (int, opcional): Número de bytes para ler para a detecção. Padrão é 10000.\n",
    "    \n",
    "    Retorna:\n",
    "        dict: Dicionário com caminho do arquivo como chave e codificação detectada como valor.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encontrar arquivos que correspondem ao padrão ou caminho fornecido.\n",
    "    files = glob.glob(file_pattern_or_path)\n",
    "    encodings = {}\n",
    "\n",
    "    # Iterar sobre cada arquivo encontrado.\n",
    "    for file_path in files:\n",
    "        # Abrir arquivo em modo binário e ler os primeiros 'num_bytes' bytes.\n",
    "        with open(file_path, 'rb') as f:\n",
    "            rawdata = f.read(num_bytes)\n",
    "            # Detectar a codificação do fragmento lido e armazenar no dicionário.\n",
    "            encodings[file_path] = chardet.detect(rawdata)[\"encoding\"]\n",
    "    \n",
    "    return encodings\n",
    "\n",
    "\n",
    "def padronizar_nome_coluna(coluna):\n",
    "    \"\"\"\n",
    "    Padroniza o nome de uma coluna removendo acentos, transformando em minúsculas e substituindo espaços, hífens, \n",
    "    parênteses, pontos, vírgulas, porcentagens e barras por underscores ou removendo-os. Remove underscores duplicados.\n",
    "\n",
    "    Parâmetros:\n",
    "    coluna (str): Nome da coluna a ser padronizado.\n",
    "\n",
    "    Retorna:\n",
    "    str: Nome da coluna padronizado.\n",
    "    \"\"\"\n",
    "    from unidecode import unidecode\n",
    "\n",
    "    # Remover acentos do nome da coluna.\n",
    "    coluna = unidecode(coluna)\n",
    "    # Transformar todas as letras em minúsculas.\n",
    "    coluna = coluna.lower()\n",
    "    # Substituir espaços, hífens, parênteses, pontos, vírgulas, porcentagens, barras e barras invertidas por underscores ou removendo-os.\n",
    "    coluna = coluna.replace(' ', '_').replace('-', '_').replace('(', '_').replace(')', '_').replace('.', '_').replace(',', '_').replace('%', 'pcnt').replace('/', '').replace('\\\\', '')\n",
    "    # Substituir underscores duplicados por um único underscore.\n",
    "    while '__' in coluna:\n",
    "        coluna = coluna.replace('__', '_')\n",
    "\n",
    "    return coluna\n",
    "\n",
    "def dms_to_decimal(dms):\n",
    "    \"\"\"\n",
    "    Converte coordenadas do formato DMS (graus, minutos, segundos) para graus decimais.\n",
    "    \n",
    "    Parâmetros:\n",
    "        dms (str): Uma string representando as coordenadas em formato DMS.\n",
    "    \n",
    "    Retorna:\n",
    "        float: O valor decimal das coordenadas fornecidas.\n",
    "    \n",
    "    Exemplos de formato DMS:\n",
    "        - \"05°56'00.0\\\"S\" -> -5.933333\n",
    "        - \"040°17'51.0\\\"W\" -> -40.297500\n",
    "    \"\"\"\n",
    "\n",
    "    # Encontrar os números (graus, minutos, segundos) e a direção (N/S/E/W)\n",
    "    numbers = re.findall(r'\\d+', dms)\n",
    "    direction = re.search(r'[NSEW]', dms).group()\n",
    "    \n",
    "    # Converter graus, minutos e segundos para float e calcular o valor decimal\n",
    "    decimal = float(numbers[0]) + float(numbers[1])/60 + float(numbers[2])/3600\n",
    "    \n",
    "    # Se a direção for Sul ou Oeste, o valor deve ser negativo\n",
    "    if direction in ['S', 'W']:\n",
    "        decimal *= -1\n",
    "    return decimal\n",
    "\n",
    "def clean_name_aero(name):\n",
    "    \"\"\"\n",
    "    Limpa e padroniza nomes de aeródromos removendo conteúdos entre parênteses,\n",
    "    acentos, caracteres especiais e convertendo para maiúsculas.\n",
    "\n",
    "    Args:\n",
    "    name (str): Nome do aeródromo a ser limpo.\n",
    "\n",
    "    Returns:\n",
    "    Series: Nome do aeródromo limpo e padronizado.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove o conteúdo dentro de parênteses.\n",
    "    # Utiliza expressões regulares para identificar texto entre parênteses e substituir por string vazia.\n",
    "    # A função strip() é usada para remover espaços em branco no início e no fim da string.\n",
    "    name = pd.Series(name).replace(r'\\(.*?\\)', '', regex=True).str.strip()\n",
    "\n",
    "    # Remove acentos e caracteres especiais.\n",
    "    # Normaliza a string para a forma de decomposição NFKD, que separa letras de acentos.\n",
    "    # Codifica a string em ASCII, ignorando erros (ignorando caracteres não ASCII).\n",
    "    # Decodifica a string de volta para UTF-8.\n",
    "    # Converte a string para letras maiúsculas.\n",
    "    return name.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8').str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9024768f-5f5f-4abd-bcd7-983519fbd10e",
   "metadata": {},
   "source": [
    "# Carregando dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2409c0f-623e-4b5e-a49e-364f7c7a5719",
   "metadata": {},
   "source": [
    "## Meteorologicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7e8587bd-8f7f-4206-99b8-b2d13aa2710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The base path where the CSV files are located\n",
    "base_path = 'DADOS_METEROLOGIA/CLIMA_2023/*.CSV'\n",
    "\n",
    "# Initialize an empty list to collect all dataframes\n",
    "all_dataframes = []\n",
    "\n",
    "# Get a list of all CSV file paths\n",
    "file_paths = glob.glob(base_path)\n",
    "\n",
    "# Process each file and append the result to the list\n",
    "for file_path in file_paths:\n",
    "    df_processed = process_file(file_path)\n",
    "    all_dataframes.append(df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "004270e8-ce69-4bf2-8eea-b34b4b8f0385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all dataframes into one\n",
    "final_dataframe = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "# Renomear colunas do DataFrame\n",
    "colunas_renomeadas = {\n",
    "     list(final_dataframe)[0]: 'data',\n",
    "    list(final_dataframe)[1]: 'hora_utc',\n",
    "    'REGI?O': 'REGIAO',\n",
    "    'ESTAC?O': 'ESTACAO',\n",
    "    'DATA DE FUNDAC?O': 'DATA DE FUNDACAO'\n",
    "}\n",
    "\n",
    "final_dataframe.rename(columns=colunas_renomeadas, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7ebf71f8-ad42-4c0a-97bb-fbbd756e92fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar a função a cada nome de coluna\n",
    "final_dataframe.columns = [padronizar_nome_coluna(col) for col in final_dataframe.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4aede891-8590-4af8-bce4-f1e26504fee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframe = final_dataframe[['data', 'hora_utc',\n",
    " 'precipitacao_total_horario_mm_', 'pressao_atmosferica_ao_nivel_da_estacao_horaria_mb_',\n",
    " 'pressao_atmosferica_max_na_hora_ant_aut_mb_', 'pressao_atmosferica_min_na_hora_ant_aut_mb_',\n",
    " 'temperatura_do_ar_bulbo_seco_horaria_degc_', 'temperatura_do_ponto_de_orvalho_degc_',\n",
    " 'temperatura_maxima_na_hora_ant_aut_degc_', 'temperatura_minima_na_hora_ant_aut_degc_',\n",
    " 'temperatura_orvalho_max_na_hora_ant_aut_degc_', 'temperatura_orvalho_min_na_hora_ant_aut_degc_',\n",
    " 'umidade_rel_max_na_hora_ant_aut_pcnt_', 'umidade_rel_min_na_hora_ant_aut_pcnt_',\n",
    " 'umidade_relativa_do_ar_horaria_pcnt_', 'vento_direcao_horaria_gr_deg_gr_',\n",
    " 'vento_rajada_maxima_ms_', 'vento_velocidade_horaria_ms_','regiao',\n",
    " 'uf', 'estacao', 'altitude']]\n",
    "\n",
    "final_dataframe[list(final_dataframe)[1]] = final_dataframe[list(final_dataframe)[1]].str[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6550b572-79f4-41f0-a3b7-af03afd6d1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove accents and special characters, and to clean up names\n",
    "def clean_name(name):\n",
    "    # Remove content inside parentheses\n",
    "    name = pd.Series(name).replace(r'\\(.*?\\)', '', regex=True).str.strip()\n",
    "    # Remove content after a hyphen\n",
    "    name = name.replace(r'\\-.*', '', regex=True).str.strip()\n",
    "    # Remove accents and special characters\n",
    "    return name.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "\n",
    "# Applying the cleaning function to the 'ESTACAO' column\n",
    "final_dataframe['estacao'] = clean_name(final_dataframe['estacao'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "08f70e0f-4305-485b-b57f-4653187182c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o caminho do diretório e do arquivo CSV\n",
    "diretorio = 'DADOS_METEROLOGIA/DADOS_METEOROLOGICOS_TRATADOS/'\n",
    "nome_arquivo = 'dados_meteorologicos_2023.parquet'\n",
    "caminho_completo = os.path.join(diretorio, nome_arquivo)\n",
    "\n",
    "# Verificar se o diretório existe. Se não, criar o diretório\n",
    "if not os.path.exists(diretorio):\n",
    "    os.makedirs(diretorio)\n",
    "\n",
    "# Lista das colunas que devem ser convertidas para float64\n",
    "colunas_para_converter = [\n",
    " 'precipitacao_total_horario_mm_',\n",
    " 'pressao_atmosferica_ao_nivel_da_estacao_horaria_mb_',\n",
    " 'pressao_atmosferica_max_na_hora_ant_aut_mb_',\n",
    " 'pressao_atmosferica_min_na_hora_ant_aut_mb_',\n",
    " 'temperatura_do_ar_bulbo_seco_horaria_degc_',\n",
    " 'temperatura_do_ponto_de_orvalho_degc_',\n",
    " 'temperatura_maxima_na_hora_ant_aut_degc_',\n",
    " 'temperatura_minima_na_hora_ant_aut_degc_',\n",
    " 'temperatura_orvalho_max_na_hora_ant_aut_degc_',\n",
    " 'temperatura_orvalho_min_na_hora_ant_aut_degc_',\n",
    " 'umidade_rel_max_na_hora_ant_aut_pcnt_',\n",
    " 'umidade_rel_min_na_hora_ant_aut_pcnt_',\n",
    " 'umidade_relativa_do_ar_horaria_pcnt_',\n",
    " 'vento_direcao_horaria_gr_deg_gr_',\n",
    " 'vento_rajada_maxima_ms_',\n",
    " 'vento_velocidade_horaria_ms_']\n",
    "\n",
    "# Converter cada coluna problemática para float64, substituindo vírgulas por pontos\n",
    "for coluna in colunas_para_converter:\n",
    "    if coluna in final_dataframe.columns:\n",
    "        # Verifica se a coluna é do tipo 'object', o que geralmente indica strings em pandas\n",
    "        if final_dataframe[coluna].dtype == 'object':\n",
    "            final_dataframe[coluna] = final_dataframe[coluna].str.replace(',', '.')\n",
    "        \n",
    "        # Converte a coluna para numérico, tratando erros como 'coerce'\n",
    "        final_dataframe[coluna] = pd.to_numeric(final_dataframe[coluna], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a0d4aa33-43d8-41ba-bfa4-12cc209c892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframe[list(final_dataframe)[0]] = final_dataframe[list(final_dataframe)[0]].str.replace('/', '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "47adc3ff-edbc-461e-bc2d-79e813575c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframe = final_dataframe.groupby(['data', 'hora_utc', 'estacao', 'uf', 'regiao']).mean()\n",
    "\n",
    "final_dataframe.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "749d6b55-09c9-425b-abe8-7bade48c9bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo Parquet salvo com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# Tente salvar o DataFrame como um arquivo Parquet novamente\n",
    "try:\n",
    "    final_dataframe.to_parquet(caminho_completo)\n",
    "    print(\"Arquivo Parquet salvo com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao salvar o arquivo Parquet: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aac3885-10ed-451e-9b02-49ee6a59bf58",
   "metadata": {},
   "source": [
    "## Aeroportos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5b9ae6a1-d487-41b5-83e5-dfcafb8a69bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Carregamento dos Dados dos Aeroportos ---\n",
    "# Definindo o caminho para o arquivo CSV dos glossários de aeródromos.\n",
    "file_path = \"dados_complementares/aerodromos_brasil.csv\"\n",
    "\n",
    "# Detectando a codificação do arquivo de aeródromos para assegurar a leitura correta dos dados.\n",
    "file_encodings = detect_encoding(file_path)\n",
    "\n",
    "# Obtendo a codificação detectada para o arquivo específico.\n",
    "encoding = file_encodings[file_path]\n",
    "\n",
    "# Lendo o arquivo CSV com a codificação apropriada.\n",
    "# A codificação detectada é usada para lidar com possíveis caracteres especiais nos dados.\n",
    "df_aeroportos = pd.read_csv(file_path, sep=';', encoding=encoding, skiprows=1)  # A codificação pode variar (ex: 'iso-8859-1', 'cp1252'), dependendo do arquivo.\n",
    "\n",
    "df_aeroportos = df_aeroportos[['Código OACI', 'CIAD', 'Nome', 'Município', 'UF', \n",
    "                               'Município Servido', 'UF Servido', 'LATGEOPOINT', 'LONGEOPOINT', \n",
    "                               'Latitude', 'Longitude', 'Altitude', 'Situação']]\n",
    "\n",
    "# Converter as colunas Latitude e Longitude para graus decimais\n",
    "df_aeroportos['LATITUDE_AERO'] = df_aeroportos['Latitude'].apply(dms_to_decimal)\n",
    "df_aeroportos['LONGITUDE_AERO'] = df_aeroportos['Longitude'].apply(dms_to_decimal)\n",
    "\n",
    "# Aplicar a função a cada nome de coluna\n",
    "df_aeroportos.columns = [padronizar_nome_coluna(col) for col in df_aeroportos.columns]\n",
    "\n",
    "# Applying the cleaning function to the 'municipio' column\n",
    "df_aeroportos['municipio'] = clean_name_aero(df_aeroportos['municipio'])\n",
    "\n",
    "# Applying the cleaning function to the 'nome' column\n",
    "df_aeroportos['nome'] = clean_name_aero(df_aeroportos['nome'])\n",
    "\n",
    "# Applying the cleaning function to the 'uf' column\n",
    "df_aeroportos['uf'] = clean_name_aero(df_aeroportos['uf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2653cb23-73c4-403a-b5c0-180645eac451",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aeroportos = df_aeroportos[['codigo_oaci','ciad','nome','municipio','uf','altitude','latgeopoint','longeopoint','latitude_aero','longitude_aero']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4c5296c0-dff9-48fe-8ca3-035ef0f30389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o caminho do diretório e do arquivo CSV\n",
    "diretorio = 'dados_tratados'\n",
    "nome_arquivo = 'dados_aeroportos_tratado.csv'\n",
    "caminho_completo = os.path.join(diretorio, nome_arquivo)\n",
    "\n",
    "# Verificar se o diretório existe. Se não, criar o diretório\n",
    "if not os.path.exists(diretorio):\n",
    "    os.makedirs(diretorio)\n",
    "\n",
    "# Salvar o DataFrame no arquivo CSV\n",
    "df_aeroportos.to_csv(caminho_completo, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aeb6b0-e82c-4e7b-8321-869cb82f03ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
