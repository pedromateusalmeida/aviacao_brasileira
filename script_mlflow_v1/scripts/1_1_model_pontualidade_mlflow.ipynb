{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7966054a-5e95-45a4-8542-1a3c076bd0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae5cd081-20db-47c3-9702-8a148f0fa6f6",
   "metadata": {},
   "source": [
    "# Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60625da5-0583-4d59-adc5-1980c0b1a38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 2.1.4\n",
      "NumPy version: 1.26.2\n",
      "XGBoost version: 2.0.2\n",
      "LightGBM version: 4.4.0\n",
      "SHAP version: 0.44.0\n"
     ]
    }
   ],
   "source": [
    "# Bibliotecas padrão e manipulação de dados\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from datetime import datetime, date\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pytz import timezone\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Configurações e filtros\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Visualização de dados\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import missingno as msno\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Machine Learning - Modelos e Pré-processamento\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, RepeatedStratifiedKFold, KFold, StratifiedKFold, GridSearchCV\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, average_precision_score, classification_report, confusion_matrix, f1_score,\n",
    "    log_loss, precision_recall_curve, precision_score, recall_score, roc_auc_score, roc_curve, auc,\n",
    "    balanced_accuracy_score, brier_score_loss, cohen_kappa_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_selection import (\n",
    "    VarianceThreshold, RFE, SelectFromModel, SequentialFeatureSelector, mutual_info_classif, mutual_info_regression\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from boruta import BorutaPy\n",
    "\n",
    "# Estatística e testes de hipótese\n",
    "from scipy.stats import (\n",
    "    chi2_contingency, kruskal, ks_2samp, fisher_exact, mannwhitneyu, power_divergence\n",
    ")\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Modelos avançados e otimização\n",
    "from hyperopt import fmin, tpe, Trials, hp, STATUS_OK\n",
    "from hyperopt.pyll import scope\n",
    "from skopt import forest_minimize\n",
    "\n",
    "# Avaliação de modelos e explanação\n",
    "from shap import Explainer\n",
    "\n",
    "# Salvamento e carregamento de modelos com MLflow\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import mlflow.lightgbm\n",
    "import mlflow.catboost\n",
    "\n",
    "# Simulação de Dataset (Substitua pelo seu)\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Impressão de versões das bibliotecas utilizadas\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "#print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(f\"LightGBM version: {lgb.__version__}\")\n",
    "#print(f\"CatBoost version: {CatBoostClassifier.__module__.split('.')[0]} version: {ctb.__version__}\")\n",
    "print(f\"SHAP version: {shap.__version__}\")\n",
    "#print(f\"PPScore version: {pps.__version__}\")\n",
    "#print(f\"missingno version: {msno.__version__}\")\n",
    "#print(f\"MLflow version: {mlflow.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5e9464-acc9-4965-a8bb-7d96fb4a47b7",
   "metadata": {},
   "source": [
    "# Criando ou carregando o experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b06438f-a6cf-4384-ac22-b50c638b20ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O experimento 'Teste XGBoost MLflow Aviação' já existe.\n"
     ]
    }
   ],
   "source": [
    "# Nome do experimento que você deseja verificar/criar\n",
    "experiment_name = \"Teste XGBoost MLflow Aviação\"\n",
    "\n",
    "# Verificar se o experimento já existe\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "# Se o experimento não existir, cria-o\n",
    "if experiment is None:\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    print(f\"O experimento '{experiment_name}' foi criado.\")\n",
    "else:\n",
    "    print(f\"O experimento '{experiment_name}' já existe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e48bb255-be0c-4edd-9716-071c78de0fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O experimento id é:'335583458888979155'\n"
     ]
    }
   ],
   "source": [
    "# Verificar se o experimento já existe\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "# Id do experimento\n",
    "experiment_id = experiment.experiment_id\n",
    "print(f\"O experimento id é:'{experiment_id}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64091f7-8406-4fe7-9720-a2c8aeeed332",
   "metadata": {},
   "source": [
    "# Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7af7e6-74b2-4721-8388-f909e893d779",
   "metadata": {},
   "source": [
    "## Carregando Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81700229-219d-4b6b-bc90-6d132e4a4ff7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Desenvolvimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398653a4-5ca1-40aa-bbc9-bd05ccd69017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo o caminho do arquivo CSV que contém os dados históricos de voos.\n",
    "file_path = 'df_treinamento_oos.csv'\n",
    "        \n",
    "# Lendo o arquivo CSV e carregando os dados em um DataFrame do pandas.\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce66f36-7f86-47d6-9f33-4247628a8733",
   "metadata": {},
   "source": [
    "## No mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "765188ba-2498-469f-aadf-a8120e615e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start an MLflow run context\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name='extração e tratamento dos dados', \n",
    "                      description = 'Extração e/ou tratamento de dados',\n",
    "                      tags = {\"Extração\": \"origem_x\", \"objetivo\": \"alimentar o modelo_x\", \"Versão da etapa\": \"1.0\"}):\n",
    "    # Carregamento de dados históricos de voos a partir de um arquivo CSV.\n",
    "    # Definindo o caminho do arquivo CSV que contém os dados históricos de voos.\n",
    "    file_path = 'df_treinamento_oos.csv'\n",
    "        \n",
    "    # Lendo o arquivo CSV e carregando os dados em um DataFrame do pandas.\n",
    "    df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadd1211-adb5-412f-9868-dc01f94e5cdb",
   "metadata": {},
   "source": [
    "## Pre processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4163e70c-3d31-4f99-80cb-82b38a0ea7ca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Desenvolvimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b36e625-8a0c-4e33-850b-8bb67755fed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns =[ 'codigo_di', 'codigo_tipo_linha'])\n",
    "\n",
    "df = df[list(df)]\n",
    "\n",
    "list_dummies =  colunas_categ = df.drop(columns = 'status_do_voo').select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Transformar colunas categóricas em tipo \"category\"\n",
    "df[list_dummies] = df[list_dummies].astype(\"category\")\n",
    "\n",
    "# Seleção das features preditoras (X) e variável-alvo (y)\n",
    "dt_ax = df.drop(columns=[\"status_do_voo\"])\n",
    "dt_ay = df[['status_do_voo']]\n",
    "\n",
    "# Codificação da variável-alvo\n",
    "label_mapping = {'Pontual': 0, 'Atrasado': 1}\n",
    "dt_ay = dt_ay['status_do_voo'].map(label_mapping)\n",
    "\n",
    "# Codifica colunas categóricas como inteiros\n",
    "label_encoders = {}\n",
    "for col in list_dummies:\n",
    "    le = LabelEncoder()\n",
    "    dt_ax[col] = le.fit_transform(dt_ax[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Segmentação em treino (86%) e teste (14,20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(dt_ax, dt_ay, random_state=33, test_size=0.142)\n",
    "\n",
    "# Segmentação adicional para validação/calibração (84,5% treino / 16,5% calibração)\n",
    "X_train_valid, X_test_valid, y_train_valid, y_test_valid = train_test_split(X_train, y_train, random_state=33, test_size=0.165)\n",
    "\n",
    "# Reverter os valores transformados para o tipo \"category\" original\n",
    "def revert_to_category(data, label_encoders, list_dummies):\n",
    "    for col in list_dummies:\n",
    "        if col in data.columns:\n",
    "            le = label_encoders[col]\n",
    "            data[col] = le.inverse_transform(data[col])\n",
    "    return data\n",
    "\n",
    "# Aplicar a reversão em X_smote_a, X_test_calib, X_test\n",
    "X_train_valid = revert_to_category(X_train_valid, label_encoders, list_dummies)\n",
    "X_test_valid = revert_to_category(X_test_valid, label_encoders, list_dummies)\n",
    "X_test = revert_to_category(X_test, label_encoders, list_dummies)\n",
    "\n",
    "# Para garantir que as colunas estão no tipo \"category\"\n",
    "X_train_valid[list_dummies] = X_train_valid[list_dummies].astype(\"category\")\n",
    "X_test_valid[list_dummies] = X_test_valid[list_dummies].astype(\"category\")\n",
    "X_test[list_dummies] = X_test[list_dummies].astype(\"category\")\n",
    "\n",
    "# Converte os nomes das colunas para uma lista de strings\n",
    "feature_names = list(X_test.columns)\n",
    "\n",
    "# Converte os conjuntos para DMatrix\n",
    "dtrain = xgb.DMatrix(X_train_valid, label=y_train_valid, enable_categorical=True, feature_names=feature_names, nthread=-1)\n",
    "dtest_valid = xgb.DMatrix(X_test_valid, label=y_test_valid, enable_categorical=True, feature_names=feature_names, nthread=-1)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test, enable_categorical=True, feature_names=feature_names, nthread=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a4966a-c2f3-411f-b273-62d93c73dd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(X_train_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(X_test_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f25c094-e25b-4bfd-88d2-46229bf02160",
   "metadata": {},
   "source": [
    "### Mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bfe8356-8f8f-4487-b42a-661c8cda5f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para calcular e registrar a distribuição de classes\n",
    "def log_class_distribution(y, label):\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    distribution = dict(zip(unique, counts))\n",
    "    total = sum(counts)\n",
    "    mlflow.log_param(f\"{label}_class_distribution\", {f\"Class {k}\": f\"{v/total:.2%}\" for k, v in distribution.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32fa4d6e-4552-4f78-a326-b15db87554fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(experiment_id=experiment_id, run_name='Pre-processamento',\n",
    "                      nested=True,\n",
    "                      description='Garantir o input correto dos modelos',\n",
    "                      tags={\"Pre-processamento\": \"preparação para treinamento\", \"objetivo\": \"garantir o input correto dos dados\", \"Versão da etapa\": \"1.0\"}):\n",
    "\n",
    "    # Etapa 1: Exclusão de colunas desnecessárias\n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name='drop columns', nested=True, \n",
    "                          description='Exclusão de colunas desnecessárias',\n",
    "                          tags={\"Tratamento\": \"drop_columns\"}):\n",
    "        df = df.drop(columns=['codigo_di', 'codigo_tipo_linha'])\n",
    "        mlflow.log_param(\"colunas_excluidas\", ['codigo_di', 'codigo_tipo_linha'])\n",
    "\n",
    "    # Etapa 2: Transformar colunas categóricas em tipo \"category\"\n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name='Transformar colunas categóricas', nested=True, \n",
    "                          description='Converte colunas categóricas para o tipo category',\n",
    "                          tags={\"Tratamento\": \"category_conversion\"}):\n",
    "        list_dummies = df.drop(columns='status_do_voo').select_dtypes(include=['object']).columns.tolist()\n",
    "        df[list_dummies] = df[list_dummies].astype(\"category\")\n",
    "        mlflow.log_param(\"colunas_categoricas\", list_dummies)\n",
    "\n",
    "    # Etapa 3: Seleção de features e variável-alvo\n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name='Seleção de features', nested=True, \n",
    "                          description='Selecionar features preditoras e variável-alvo',\n",
    "                          tags={\"Tratamento\": \"feature_selection\"}):\n",
    "        dt_ax = df.drop(columns=[\"status_do_voo\"])\n",
    "        dt_ay = df['status_do_voo'].map({'Pontual': 0, 'Atrasado': 1})\n",
    "        mlflow.log_param(\"target_mapping\", {'Pontual': 0, 'Atrasado': 1})\n",
    "        mlflow.log_param(\"n_features\", dt_ax.shape[1])\n",
    "\n",
    "    # Etapa 4: Codificação de colunas categóricas\n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name='Codificação de colunas categóricas', nested=True, \n",
    "                          description='Codificar colunas categóricas como inteiros',\n",
    "                          tags={\"Tratamento\": \"label_encoding\"}):\n",
    "        label_encoders = {}\n",
    "        for col in list_dummies:\n",
    "            le = LabelEncoder()\n",
    "            dt_ax[col] = le.fit_transform(dt_ax[col])\n",
    "            label_encoders[col] = le\n",
    "        mlflow.log_param(\"n_label_encoded_columns\", len(list_dummies))\n",
    "\n",
    "    # Etapa 5: Segmentação em treino, teste e validação\n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name='Segmentação em treino/teste/validação', nested=True, \n",
    "                          description='Segmentação dos dados em treino (71,64%), validação (14,15%) e teste (14,20%)',\n",
    "                          tags={\"Tratamento\": \"data_split\"}):\n",
    "        # Realizar a segmentação\n",
    "        X_train, X_test, y_train, y_test = train_test_split(dt_ax, dt_ay, random_state=33, test_size=0.142)\n",
    "        X_train_valid, X_test_valid, y_train_valid, y_test_valid = train_test_split(X_train, y_train, random_state=33, test_size=0.165)\n",
    "        \n",
    "        # Registrar o tamanho dos conjuntos\n",
    "        mlflow.log_param(\"train_size\", len(X_train_valid))\n",
    "        mlflow.log_param(\"validation_size\", len(X_test_valid))\n",
    "        mlflow.log_param(\"test_size\", len(X_test))\n",
    "        \n",
    "        # Registrar a distribuição de classes\n",
    "        log_class_distribution(y_train, 'train_size')\n",
    "        log_class_distribution(y_test_valid, 'validation_size')\n",
    "        log_class_distribution(y_test, 'test_size')\n",
    "\n",
    "    # Etapa 6: Reversão e preparação final dos dados\n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name='Reversão e preparação final', nested=True, \n",
    "                          description='Reverter valores transformados para o tipo category original e preparação final',\n",
    "                          tags={\"Tratamento\": \"final_preparation\"}):\n",
    "        def revert_to_category(data, label_encoders, list_dummies):\n",
    "            for col in list_dummies:\n",
    "                if col in data.columns:\n",
    "                    le = label_encoders[col]\n",
    "                    data[col] = le.inverse_transform(data[col])\n",
    "            return data\n",
    "\n",
    "        X_train_valid = revert_to_category(X_train_valid, label_encoders, list_dummies)\n",
    "        X_test_valid = revert_to_category(X_test_valid, label_encoders, list_dummies)\n",
    "        X_test = revert_to_category(X_test, label_encoders, list_dummies)\n",
    "        \n",
    "        # Garantir que as colunas estão no tipo \"category\"\n",
    "        X_train_valid[list_dummies] = X_train_valid[list_dummies].astype(\"category\")\n",
    "        X_test_valid[list_dummies] = X_test_valid[list_dummies].astype(\"category\")\n",
    "        X_test[list_dummies] = X_test[list_dummies].astype(\"category\")\n",
    "        \n",
    "        mlflow.log_param(\"categorical_columns_finalized\", list_dummies)\n",
    "\n",
    "    # Etapa 7: Conversão para DMatrix\n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name='Conversão para DMatrix', nested=True, \n",
    "                          description='Converter conjuntos de dados para DMatrix para treinamento com XGBoost',\n",
    "                          tags={\"Tratamento\": \"dmatrix_conversion\"}):\n",
    "        feature_names = list(X_test.columns)\n",
    "        dtrain = xgb.DMatrix(X_train_valid, label=y_train_valid, enable_categorical=True, feature_names=feature_names, nthread=-1)\n",
    "        dtest_valid = xgb.DMatrix(X_test_valid, label=y_test_valid, enable_categorical=True, feature_names=feature_names, nthread=-1)\n",
    "        dtest = xgb.DMatrix(X_test, label=y_test, enable_categorical=True, feature_names=feature_names, nthread=-1)\n",
    "        mlflow.log_param(\"feature_names\", feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a45978b-6be3-4718-8f9a-9fa72d03fcc0",
   "metadata": {},
   "source": [
    "## Hipertunnig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc21ad8-2034-4fdd-a37f-03df93490921",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Hipertunnig desenvolvimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9479ff4-c1e6-481e-8baf-1e7f8554cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hipertunnig(space):\n",
    "    \"\"\"\n",
    "    Realiza o ajuste de hiperparâmetros de um modelo XGBoost usando validação cruzada com DMatrix.\n",
    "    \n",
    "    Args:\n",
    "    space (dict): Dicionário contendo os hiperparâmetros avaliados pelo Hyperopt.\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dicionário contendo o 'loss' (negativo da média do AUCPR), o 'status' e as métricas adicionais (AUC).\n",
    "    \"\"\"\n",
    "    # Configuração do modelo com os parâmetros do espaço\n",
    "    params = {\n",
    "        'max_depth': int(space['max_depth']),                    # Profundidade máxima da árvore\n",
    "        'gamma': space['gamma'],                                 # Redução mínima de perda necessária para dividir um nó\n",
    "        'reg_alpha': space['reg_alpha'],                         # Termo de regularização L1 para evitar overfitting\n",
    "        'reg_lambda': space['reg_lambda'],                       # Termo de regularização L2 para evitar overfitting\n",
    "        'min_child_weight': int(space['min_child_weight']),      # Peso mínimo de instâncias em um nó filho\n",
    "        'colsample_bytree': space['colsample_bytree'],           # Proporção de colunas amostradas por árvore\n",
    "        'colsample_bylevel': space['colsample_bylevel'],         # Subamostragem de colunas por nível\n",
    "        'colsample_bynode': space['colsample_bynode'],           # Subamostragem de colunas por nó\n",
    "        'n_estimators': space['n_estimators'],                   # Número de árvores no modelo\n",
    "        'learning_rate': space['learning_rate'],                 # Taxa de aprendizado para encolher as atualizações\n",
    "        'max_delta_step': space['max_delta_step'],               # Etapa máxima para atualizar valores das folhas\n",
    "        'subsample': space['subsample'],                         # Proporção de amostragem das instâncias de treinamento\n",
    "        'sampling_method': space['sampling_method'],             # Método de amostragem (dá prioridade a gradientes maiores)\n",
    "        'tree_method': space['tree_method'],                     # Método de construção da árvore\n",
    "        'device': space['device'],                               # Dispositivo usado para treinamento (GPU)\n",
    "        'enable_categorical': space['enable_categorical'],       # Habilita suporte a dados categóricos nativamente\n",
    "        'scale_pos_weight': space['scale_pos_weight'],           # Ajusta o peso das classes desbalanceadas\n",
    "        'eval_metric': space['eval_metric'],                     # Métricas de avaliação\n",
    "        'objective': space['objective'],                         # Objetivo do modelo\n",
    "        'seed': space['seed'],                                   # Semente para reprodutibilidade dos resultados\n",
    "        'max_cat_to_onehot': int(space['max_cat_to_onehot']),    # Limite para usar one-hot em categorias\n",
    "        'max_cat_threshold': int(space['max_cat_threshold']),    # Máximo de categorias consideradas por divisão\n",
    "        'max_leaves': int(space['max_leaves']),                  # Número máximo de folhas permitidas por árvore\n",
    "        'validate_parameters': space['validate_parameters'],     # Valida os parâmetros antes de iniciar o treinamento\n",
    "        'max_bin': space['max_bin'],     # Valida os parâmetros antes de iniciar o treinamento\n",
    "        'updater': space['updater']                              # Atualizador usado para crescimento de árvores em GPU\n",
    "    }\n",
    "\n",
    "\n",
    "    print(\"Hiperparâmetros utilizados:\", params)\n",
    "    print(\"Hiperparâmetros n_estimators:\", space['n_estimators'])\n",
    "\n",
    "    \n",
    "    # Realiza a validação cruzada com xgb.cv\n",
    "    cv_results = xgb.cv(\n",
    "        params=params,\n",
    "        dtrain=dtrain,                  # DMatrix preparado\n",
    "        num_boost_round=int(space['n_estimators']),\n",
    "        nfold=5,                              # Número de folds\n",
    "        metrics=[\"aucpr\", \"auc\", \"logloss\"],  # Métricas de avaliação\n",
    "        as_pandas=True,                       # Retorna os resultados como DataFrame\n",
    "        seed=33,\n",
    "        stratified=True,                      # Garante estratificação\n",
    "        early_stopping_rounds = 20 if params['max_depth'] <= 6 else 50,           # Ativa a parada antecipada\n",
    "    )\n",
    "    \n",
    "    # Calcula a média do AUCPR e do AUC\n",
    "    mean_aucpr = cv_results[\"test-aucpr-mean\"].max()\n",
    "    mean_auc = cv_results[\"test-auc-mean\"].max()\n",
    "    mean_logloss = cv_results[\"test-logloss-mean\"].min()\n",
    "\n",
    "    print(\"Média AUCPR: \", mean_aucpr)\n",
    "    print(\"Média AUC: \", mean_auc)\n",
    "    print(\"Média LogLoss: \", mean_logloss)\n",
    "    \n",
    "    print(\"------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    print(\"Novo modelo\")\n",
    "    \n",
    "    # Retorna os resultados\n",
    "    return {\n",
    "        'loss': mean_logloss,   \n",
    "        'status': STATUS_OK,\n",
    "        'additional_metrics': {\n",
    "            'aucpr': mean_aucpr,\n",
    "            'logloss': mean_logloss,\n",
    "            'auc': mean_auc}}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c447e-a019-41dc-9308-31e96161167d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Executando a otimização\n",
    "trials = Trials()\n",
    "best_hyperparams = fmin(fn=hipertunnig, \n",
    "                        space=space, \n",
    "                        algo=tpe.suggest, \n",
    "                        max_evals=20, \n",
    "                        trials=trials)\n",
    "\n",
    "# Obtendo os melhores hiperparâmetros\n",
    "best_hyperparams = space_eval(space, best_hyperparams)\n",
    "print(\"Melhores hiperparâmetros:\", best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8c8fbc-0cb4-4ad6-af30-9ce2b83af780",
   "metadata": {},
   "source": [
    "### Hipertunning Mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7cf9997-2e2f-48df-a200-08de74e6d7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hipertunnig(space):\n",
    "    \"\"\"\n",
    "    Realiza o ajuste de hiperparâmetros de um modelo XGBoost usando validação cruzada com DMatrix.\n",
    "    \n",
    "    Args:\n",
    "    space (dict): Dicionário contendo os hiperparâmetros avaliados pelo Hyperopt.\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dicionário contendo o 'loss', o 'status' e métricas adicionais.\n",
    "    \"\"\"\n",
    "    mlflow.xgboost.autolog()\n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name='XGBoost Model Training and Tuning', nested=True):\n",
    "    # Configuração do modelo com os parâmetros do espaço\n",
    "        params = {\n",
    "            'max_depth': int(space['max_depth']),\n",
    "            'gamma': space['gamma'],\n",
    "            'reg_alpha': space['reg_alpha'],\n",
    "            'reg_lambda': space['reg_lambda'],\n",
    "            'min_child_weight': int(space['min_child_weight']),\n",
    "            'colsample_bytree': space['colsample_bytree'],\n",
    "            'colsample_bylevel': space['colsample_bylevel'],\n",
    "            'colsample_bynode': space['colsample_bynode'],\n",
    "            'n_estimators': space['n_estimators'],\n",
    "            'learning_rate': space['learning_rate'],\n",
    "            'max_delta_step': space['max_delta_step'],\n",
    "            'subsample': space['subsample'],\n",
    "            'sampling_method': space['sampling_method'],\n",
    "            'tree_method': space['tree_method'],\n",
    "            'device': space['device'],\n",
    "            'enable_categorical': space['enable_categorical'],\n",
    "            'scale_pos_weight': space['scale_pos_weight'],\n",
    "            'eval_metric': space['eval_metric'],\n",
    "            'objective': space['objective'],\n",
    "            'seed': space['seed'],\n",
    "            'max_cat_to_onehot': int(space['max_cat_to_onehot']),\n",
    "            'max_cat_threshold': int(space['max_cat_threshold']),\n",
    "            'max_leaves': int(space['max_leaves']),\n",
    "            'validate_parameters': space['validate_parameters'],\n",
    "            'max_bin': space['max_bin'],\n",
    "            'updater': space['updater']\n",
    "        }\n",
    "    \n",
    "        # Log dos parâmetros no MLflow\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "    \n",
    "        # Realiza a validação cruzada com xgb.cv\n",
    "        cv_results = xgb.cv(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=int(space['n_estimators']),\n",
    "            nfold=5,\n",
    "            metrics=[\"aucpr\", \"auc\", \"logloss\"],\n",
    "            as_pandas=True,\n",
    "            seed=33,\n",
    "            stratified=True,\n",
    "            early_stopping_rounds=15 if params['max_depth'] <= 6 else 45,\n",
    "        )\n",
    "        \n",
    "        # Captura as listas completas para cada métrica\n",
    "        aucpr_list = cv_results[\"test-aucpr-mean\"].tolist()\n",
    "        auc_list = cv_results[\"test-auc-mean\"].tolist()\n",
    "        logloss_list = cv_results[\"test-logloss-mean\"].tolist()\n",
    "    \n",
    "        # Captura as médias das métricas\n",
    "        mean_aucpr = max(aucpr_list)\n",
    "        mean_auc = max(auc_list)\n",
    "        mean_logloss = min(logloss_list)\n",
    "    \n",
    "        # Log das métricas no MLflow\n",
    "        mlflow.log_metric(\"mean_aucpr\", mean_aucpr)\n",
    "        mlflow.log_metric(\"mean_auc\", mean_auc)\n",
    "        mlflow.log_metric(\"mean_logloss\", mean_logloss)\n",
    "    \n",
    "        # Retorna os resultados\n",
    "        return {\n",
    "            'loss': mean_logloss,  # Minimiza logloss\n",
    "            'status': STATUS_OK,\n",
    "            'additional_metrics': {\n",
    "                'aucpr': mean_aucpr,\n",
    "                'auc': mean_auc,\n",
    "                'logloss': mean_logloss,\n",
    "                'aucpr_list': aucpr_list,\n",
    "                'auc_list': auc_list,\n",
    "                'logloss_list': logloss_list,\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8aba708-df40-422d-87e8-a2818055eddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Espaço de busca atualizado\n",
    "space = {\n",
    "    'max_depth': scope.int(hp.quniform(\"max_depth\", 2, 35, 1)),                                      # Profundidade máxima da árvore\n",
    "    'gamma': hp.uniform('gamma', 1, 20),                                                             # Redução mínima de perda necessária para dividir um nó\n",
    "    'reg_alpha': scope.int(hp.quniform('reg_alpha', 0, 200, 1)),                                     # Termo de regularização L1 para evitar overfitting\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 20),                                                   # Termo de regularização L2 para evitar overfitting\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 0.9),                                    # Proporção de colunas amostradas por árvore\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.3, 1),                                    # Subamostragem de colunas por nível\n",
    "    'colsample_bynode': hp.uniform('colsample_bynode', 0.3, 1),                                      # Subamostragem de colunas por nó\n",
    "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 30, 1)),                        # Peso mínimo de instâncias em um nó filho\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.5),                                         # Taxa de aprendizado para encolher as atualizações\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 50, 1000, 10)),                            # Número de árvores no modelo\n",
    "    'max_delta_step': hp.uniform('max_delta_step', 0, 15),                                           # Etapa máxima para atualizar valores das folhas\n",
    "    'subsample': hp.uniform('subsample', 0.4, 1),                                                    # Proporção de amostragem das instâncias de treinamento\n",
    "    'sampling_method': 'gradient_based',                                                             # Método de amostragem (dá prioridade a gradientes maiores)\n",
    "    'tree_method': 'hist',                                                                           # Método de construção da árvore (histograma otimizado para CPU)\n",
    "    'device': 'cuda',                                                                                # Dispositivo usado para treinamento (GPU)\n",
    "    'enable_categorical': True,                                                                      # Habilita suporte a dados categóricos nativamente\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 4, 40),                                       # Ajusta o peso das classes desbalanceadas\n",
    "    'max_cat_to_onehot': scope.int(hp.quniform('max_cat_to_onehot', 3, 70, 1)),                      # Limite para usar one-hot em categorias\n",
    "    'max_cat_threshold': scope.int(hp.quniform('max_cat_threshold', 3, 70, 1)),                      # Máximo de categorias consideradas por divisão\n",
    "    #'grow_policy': hp.choice('grow_policy', ['depthwise', 'lossguide']),                             # Estratégia de crescimento das árvores\n",
    "    'max_leaves': scope.int(hp.quniform('max_leaves', 16, 256, 4)),                                  # Número máximo de folhas permitidas por árvore\n",
    "    'validate_parameters': True,                                                                     # Valida os parâmetros antes de iniciar o treinamento\n",
    "    'seed': 33,                                                                                      # Semente para reprodutibilidade dos resultados\n",
    "    'eval_metric': [\"aucpr\", \"auc\", \"logloss\", 'error'],                                                      # Métricas de avaliação para otimizar\n",
    "    'updater': 'grow_gpu_hist',                                                                      # Atualizador usado para crescimento de árvores em GPU\n",
    "    'max_bin': scope.int(hp.quniform('max_bin', 32, 320, 8)),                                      # Número máximo de bins discretos para histogramas\n",
    "    # 'multi_strategy': hp.choice('multi_strategy', ['one_output_per_tree', 'multi_output_tree']),     # Estratégia para múltiplos alvos\n",
    "    #'num_parallel_tree': scope.int(hp.quniform('num_parallel_tree', 1, 10, 1)),                      # Número de árvores paralelas em cada iteração\n",
    "    #'process_type': hp.choice('process_type', ['default', 'update']),                               # Tipo de processo de aprendizado\n",
    "    'objective': 'binary:logistic'                                                                   # Objetivo de aprendizado (classificação binária)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "519ef5b4-f486-4b6c-9f73-f12564b53a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 50/50 [22:11<00:00, 26.62s/trial, best loss: 0.5356836140514301]\n",
      "Melhores hiperparâmetros: {'colsample_bylevel': 0.6952308158892839, 'colsample_bynode': 0.4519555761806504, 'colsample_bytree': 0.5865095330714133, 'gamma': 14.627209827110498, 'learning_rate': 0.07479272954025266, 'max_bin': 56.0, 'max_cat_threshold': 59.0, 'max_cat_to_onehot': 27.0, 'max_delta_step': 4.302907171260744, 'max_depth': 17.0, 'max_leaves': 88.0, 'min_child_weight': 3.0, 'n_estimators': 800.0, 'reg_alpha': 41.0, 'reg_lambda': 7.549327551534676, 'scale_pos_weight': 4.102779361039333, 'subsample': 0.48162810550671653}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'space_eval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 16\u001b[0m\n\u001b[0;32m      8\u001b[0m best_hyperparams \u001b[38;5;241m=\u001b[39m fmin(fn\u001b[38;5;241m=\u001b[39mhipertunnig,\n\u001b[0;32m      9\u001b[0m                         space\u001b[38;5;241m=\u001b[39mspace,\n\u001b[0;32m     10\u001b[0m                         algo\u001b[38;5;241m=\u001b[39mtpe\u001b[38;5;241m.\u001b[39msuggest,\n\u001b[0;32m     11\u001b[0m                         max_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m     12\u001b[0m                         trials\u001b[38;5;241m=\u001b[39mtrials)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMelhores hiperparâmetros:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_hyperparams)\n\u001b[1;32m---> 16\u001b[0m best_hyperparams \u001b[38;5;241m=\u001b[39m \u001b[43mspace_eval\u001b[49m(space, best_hyperparams)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Log dos melhores parâmetros no MLflow\u001b[39;00m\n\u001b[0;32m     18\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mlog_params(best_hyperparams)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'space_eval' is not defined"
     ]
    }
   ],
   "source": [
    "# Etapa de hipertuning\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name='Hipertunning XGBoost', nested=True,\n",
    "                      description='Busca pelos melhores parâmetros. Os modelos testados são armazenados, mesmo que não tenham os melhores parâmetros.',\n",
    "                      tags={\"Hipertunning\": \"Melhores parâmetros\", \"objetivo\": \"garantir os melhores parâmetros para o modelo\"}):\n",
    "\n",
    "    # Espaço de busca\n",
    "    trials = Trials()\n",
    "    best_hyperparams = fmin(fn=hipertunnig,\n",
    "                            space=space,\n",
    "                            algo=tpe.suggest,\n",
    "                            max_evals=2,\n",
    "                            trials=trials)\n",
    "\n",
    "    print(\"Melhores hiperparâmetros:\", best_hyperparams)\n",
    "    \n",
    "\n",
    "    # Log dos melhores parâmetros no MLflow\n",
    "    mlflow.log_params(best_hyperparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95067edf-5669-42b1-9acb-588d925d3c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparâmetros: {'colsample_bylevel': 0.6952308158892839, 'colsample_bynode': 0.4519555761806504, 'colsample_bytree': 0.5865095330714133, 'gamma': 14.627209827110498, 'learning_rate': 0.07479272954025266, 'max_bin': 56.0, 'max_cat_threshold': 59.0, 'max_cat_to_onehot': 27.0, 'max_delta_step': 4.302907171260744, 'max_depth': 17.0, 'max_leaves': 88.0, 'min_child_weight': 3.0, 'n_estimators': 800.0, 'reg_alpha': 41.0, 'reg_lambda': 7.549327551534676, 'scale_pos_weight': 4.102779361039333, 'subsample': 0.48162810550671653}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4c32e1b-a5ac-4357-bd6d-6489b776c651",
   "metadata": {},
   "source": [
    "## Treinamento final do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5542e296-cd33-424c-878c-729bbb4adf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'max_depth': int(best_hyperparams['max_depth']),\n",
    "    'n_estimators': int(best_hyperparams['n_estimators']),\n",
    "    'reg_lambda': float(best_hyperparams['reg_lambda']),\n",
    "    'reg_alpha': float(best_hyperparams['reg_alpha']),\n",
    "    'gamma': float(best_hyperparams['gamma']),\n",
    "    'min_child_weight': int(best_hyperparams['min_child_weight']),\n",
    "    'colsample_bytree': float(best_hyperparams['colsample_bytree']),\n",
    "    'colsample_bylevel': float(best_hyperparams['colsample_bylevel']),\n",
    "    'colsample_bynode': float(best_hyperparams['colsample_bynode']),\n",
    "    'learning_rate': float(best_hyperparams['learning_rate']),\n",
    "    'max_delta_step': float(best_hyperparams.get('max_delta_step', 0.0)),\n",
    "    'subsample': float(best_hyperparams['subsample']),\n",
    "    'sampling_method': best_hyperparams.get('sampling_method', 'gradient_based'),\n",
    "    'tree_method': best_hyperparams.get('tree_method', 'hist'),\n",
    "    'scale_pos_weight': float(best_hyperparams['scale_pos_weight']),\n",
    "    'max_cat_to_onehot': int(best_hyperparams.get('max_cat_to_onehot', 10)),\n",
    "    'max_cat_threshold': int(best_hyperparams.get('max_cat_threshold', 20)),\n",
    "    'max_leaves': int(best_hyperparams.get('max_leaves', 256)),\n",
    "    'max_bin': int(best_hyperparams.get('max_bin', 256)),\n",
    "    'updater': best_hyperparams.get('updater', 'grow_gpu_hist'),\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': [\"aucpr\", \"auc\"],\n",
    "    'enable_categorical': True,\n",
    "    'validate_parameters': True,\n",
    "    'seed': int(best_hyperparams.get('seed', 33)),\n",
    "    'device': best_hyperparams.get('device', 'cuda'),\n",
    "    'verbosity': 1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45a399db-a011-4c8d-b601-6b23a28a0899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-aucpr:0.24414\tvalidation-auc:0.64553\n",
      "[1]\tvalidation-aucpr:0.26278\tvalidation-auc:0.66152\n",
      "[2]\tvalidation-aucpr:0.26188\tvalidation-auc:0.66416\n",
      "[3]\tvalidation-aucpr:0.26532\tvalidation-auc:0.66344\n",
      "[4]\tvalidation-aucpr:0.25709\tvalidation-auc:0.65723\n",
      "[5]\tvalidation-aucpr:0.25075\tvalidation-auc:0.65344\n",
      "[6]\tvalidation-aucpr:0.25375\tvalidation-auc:0.65629\n",
      "[7]\tvalidation-aucpr:0.25476\tvalidation-auc:0.65733\n",
      "[8]\tvalidation-aucpr:0.26092\tvalidation-auc:0.66033\n",
      "[9]\tvalidation-aucpr:0.26396\tvalidation-auc:0.66139\n",
      "[10]\tvalidation-aucpr:0.26444\tvalidation-auc:0.66294\n",
      "[11]\tvalidation-aucpr:0.26870\tvalidation-auc:0.66668\n",
      "[12]\tvalidation-aucpr:0.27119\tvalidation-auc:0.66962\n",
      "[13]\tvalidation-aucpr:0.27238\tvalidation-auc:0.67076\n",
      "[14]\tvalidation-aucpr:0.27278\tvalidation-auc:0.67129\n",
      "[15]\tvalidation-aucpr:0.27474\tvalidation-auc:0.67341\n",
      "[16]\tvalidation-aucpr:0.27512\tvalidation-auc:0.67410\n",
      "[17]\tvalidation-aucpr:0.27921\tvalidation-auc:0.67755\n",
      "[18]\tvalidation-aucpr:0.28053\tvalidation-auc:0.67846\n",
      "[19]\tvalidation-aucpr:0.28082\tvalidation-auc:0.67856\n",
      "[20]\tvalidation-aucpr:0.28135\tvalidation-auc:0.67935\n",
      "[21]\tvalidation-aucpr:0.28085\tvalidation-auc:0.67915\n",
      "[22]\tvalidation-aucpr:0.28315\tvalidation-auc:0.68107\n",
      "[23]\tvalidation-aucpr:0.28441\tvalidation-auc:0.68155\n",
      "[24]\tvalidation-aucpr:0.28503\tvalidation-auc:0.68183\n",
      "[25]\tvalidation-aucpr:0.28602\tvalidation-auc:0.68355\n",
      "[26]\tvalidation-aucpr:0.28728\tvalidation-auc:0.68492\n",
      "[27]\tvalidation-aucpr:0.28751\tvalidation-auc:0.68522\n",
      "[28]\tvalidation-aucpr:0.28872\tvalidation-auc:0.68670\n",
      "[29]\tvalidation-aucpr:0.29103\tvalidation-auc:0.68844\n",
      "[30]\tvalidation-aucpr:0.29195\tvalidation-auc:0.68920\n",
      "[31]\tvalidation-aucpr:0.29524\tvalidation-auc:0.69128\n",
      "[32]\tvalidation-aucpr:0.29620\tvalidation-auc:0.69209\n",
      "[33]\tvalidation-aucpr:0.29660\tvalidation-auc:0.69252\n",
      "[34]\tvalidation-aucpr:0.29737\tvalidation-auc:0.69317\n",
      "[35]\tvalidation-aucpr:0.29749\tvalidation-auc:0.69352\n",
      "[36]\tvalidation-aucpr:0.29816\tvalidation-auc:0.69394\n",
      "[37]\tvalidation-aucpr:0.29790\tvalidation-auc:0.69384\n",
      "[38]\tvalidation-aucpr:0.29880\tvalidation-auc:0.69457\n",
      "[39]\tvalidation-aucpr:0.29958\tvalidation-auc:0.69444\n",
      "[40]\tvalidation-aucpr:0.30006\tvalidation-auc:0.69492\n",
      "[41]\tvalidation-aucpr:0.30078\tvalidation-auc:0.69529\n",
      "[42]\tvalidation-aucpr:0.30406\tvalidation-auc:0.69752\n",
      "[43]\tvalidation-aucpr:0.30504\tvalidation-auc:0.69819\n",
      "[44]\tvalidation-aucpr:0.30628\tvalidation-auc:0.69894\n",
      "[45]\tvalidation-aucpr:0.30828\tvalidation-auc:0.69990\n",
      "[46]\tvalidation-aucpr:0.30841\tvalidation-auc:0.70042\n",
      "[47]\tvalidation-aucpr:0.30944\tvalidation-auc:0.70116\n",
      "[48]\tvalidation-aucpr:0.30950\tvalidation-auc:0.70116\n",
      "[49]\tvalidation-aucpr:0.30986\tvalidation-auc:0.70112\n",
      "[50]\tvalidation-aucpr:0.31083\tvalidation-auc:0.70166\n",
      "[51]\tvalidation-aucpr:0.31001\tvalidation-auc:0.70165\n",
      "[52]\tvalidation-aucpr:0.31196\tvalidation-auc:0.70272\n",
      "[53]\tvalidation-aucpr:0.31272\tvalidation-auc:0.70314\n",
      "[54]\tvalidation-aucpr:0.31352\tvalidation-auc:0.70377\n",
      "[55]\tvalidation-aucpr:0.31445\tvalidation-auc:0.70461\n",
      "[56]\tvalidation-aucpr:0.31523\tvalidation-auc:0.70531\n",
      "[57]\tvalidation-aucpr:0.31549\tvalidation-auc:0.70560\n",
      "[58]\tvalidation-aucpr:0.31639\tvalidation-auc:0.70600\n",
      "[59]\tvalidation-aucpr:0.31692\tvalidation-auc:0.70649\n",
      "[60]\tvalidation-aucpr:0.31802\tvalidation-auc:0.70693\n",
      "[61]\tvalidation-aucpr:0.31870\tvalidation-auc:0.70752\n",
      "[62]\tvalidation-aucpr:0.31931\tvalidation-auc:0.70779\n",
      "[63]\tvalidation-aucpr:0.31983\tvalidation-auc:0.70817\n",
      "[64]\tvalidation-aucpr:0.32027\tvalidation-auc:0.70846\n",
      "[65]\tvalidation-aucpr:0.32017\tvalidation-auc:0.70840\n",
      "[66]\tvalidation-aucpr:0.32037\tvalidation-auc:0.70837\n",
      "[67]\tvalidation-aucpr:0.32088\tvalidation-auc:0.70840\n",
      "[68]\tvalidation-aucpr:0.32130\tvalidation-auc:0.70873\n",
      "[69]\tvalidation-aucpr:0.32131\tvalidation-auc:0.70873\n",
      "[70]\tvalidation-aucpr:0.32124\tvalidation-auc:0.70867\n",
      "[71]\tvalidation-aucpr:0.32144\tvalidation-auc:0.70876\n",
      "[72]\tvalidation-aucpr:0.32167\tvalidation-auc:0.70898\n",
      "[73]\tvalidation-aucpr:0.32164\tvalidation-auc:0.70914\n",
      "[74]\tvalidation-aucpr:0.32202\tvalidation-auc:0.70938\n",
      "[75]\tvalidation-aucpr:0.32206\tvalidation-auc:0.70941\n",
      "[76]\tvalidation-aucpr:0.32294\tvalidation-auc:0.70983\n",
      "[77]\tvalidation-aucpr:0.32333\tvalidation-auc:0.71010\n",
      "[78]\tvalidation-aucpr:0.32353\tvalidation-auc:0.71017\n",
      "[79]\tvalidation-aucpr:0.32388\tvalidation-auc:0.71030\n",
      "[80]\tvalidation-aucpr:0.32475\tvalidation-auc:0.71069\n",
      "[81]\tvalidation-aucpr:0.32578\tvalidation-auc:0.71128\n",
      "[82]\tvalidation-aucpr:0.32578\tvalidation-auc:0.71128\n",
      "[83]\tvalidation-aucpr:0.32642\tvalidation-auc:0.71173\n",
      "[84]\tvalidation-aucpr:0.32659\tvalidation-auc:0.71195\n",
      "[85]\tvalidation-aucpr:0.32695\tvalidation-auc:0.71211\n",
      "[86]\tvalidation-aucpr:0.32727\tvalidation-auc:0.71227\n",
      "[87]\tvalidation-aucpr:0.32761\tvalidation-auc:0.71242\n",
      "[88]\tvalidation-aucpr:0.32757\tvalidation-auc:0.71244\n",
      "[89]\tvalidation-aucpr:0.32756\tvalidation-auc:0.71274\n",
      "[90]\tvalidation-aucpr:0.32792\tvalidation-auc:0.71308\n",
      "[91]\tvalidation-aucpr:0.32796\tvalidation-auc:0.71314\n",
      "[92]\tvalidation-aucpr:0.32796\tvalidation-auc:0.71314\n",
      "[93]\tvalidation-aucpr:0.32850\tvalidation-auc:0.71345\n",
      "[94]\tvalidation-aucpr:0.32855\tvalidation-auc:0.71347\n",
      "[95]\tvalidation-aucpr:0.32865\tvalidation-auc:0.71353\n",
      "[96]\tvalidation-aucpr:0.32900\tvalidation-auc:0.71375\n",
      "[97]\tvalidation-aucpr:0.32899\tvalidation-auc:0.71383\n",
      "[98]\tvalidation-aucpr:0.32973\tvalidation-auc:0.71402\n",
      "[99]\tvalidation-aucpr:0.32993\tvalidation-auc:0.71408\n",
      "[100]\tvalidation-aucpr:0.32999\tvalidation-auc:0.71413\n",
      "[101]\tvalidation-aucpr:0.33018\tvalidation-auc:0.71419\n",
      "[102]\tvalidation-aucpr:0.33031\tvalidation-auc:0.71436\n",
      "[103]\tvalidation-aucpr:0.33031\tvalidation-auc:0.71436\n",
      "[104]\tvalidation-aucpr:0.33028\tvalidation-auc:0.71427\n",
      "[105]\tvalidation-aucpr:0.33026\tvalidation-auc:0.71424\n",
      "[106]\tvalidation-aucpr:0.33014\tvalidation-auc:0.71421\n",
      "[107]\tvalidation-aucpr:0.33041\tvalidation-auc:0.71437\n",
      "[108]\tvalidation-aucpr:0.33094\tvalidation-auc:0.71453\n",
      "[109]\tvalidation-aucpr:0.33099\tvalidation-auc:0.71454\n",
      "[110]\tvalidation-aucpr:0.33096\tvalidation-auc:0.71454\n",
      "[111]\tvalidation-aucpr:0.33133\tvalidation-auc:0.71484\n",
      "[112]\tvalidation-aucpr:0.33163\tvalidation-auc:0.71513\n",
      "[113]\tvalidation-aucpr:0.33157\tvalidation-auc:0.71512\n",
      "[114]\tvalidation-aucpr:0.33200\tvalidation-auc:0.71527\n",
      "[115]\tvalidation-aucpr:0.33208\tvalidation-auc:0.71536\n",
      "[116]\tvalidation-aucpr:0.33208\tvalidation-auc:0.71537\n",
      "[117]\tvalidation-aucpr:0.33209\tvalidation-auc:0.71537\n",
      "[118]\tvalidation-aucpr:0.33212\tvalidation-auc:0.71540\n",
      "[119]\tvalidation-aucpr:0.33218\tvalidation-auc:0.71545\n",
      "[120]\tvalidation-aucpr:0.33219\tvalidation-auc:0.71546\n",
      "[121]\tvalidation-aucpr:0.33211\tvalidation-auc:0.71551\n",
      "[122]\tvalidation-aucpr:0.33210\tvalidation-auc:0.71550\n",
      "[123]\tvalidation-aucpr:0.33220\tvalidation-auc:0.71557\n",
      "[124]\tvalidation-aucpr:0.33222\tvalidation-auc:0.71560\n",
      "[125]\tvalidation-aucpr:0.33244\tvalidation-auc:0.71585\n",
      "[126]\tvalidation-aucpr:0.33277\tvalidation-auc:0.71603\n",
      "[127]\tvalidation-aucpr:0.33294\tvalidation-auc:0.71608\n",
      "[128]\tvalidation-aucpr:0.33319\tvalidation-auc:0.71626\n",
      "[129]\tvalidation-aucpr:0.33333\tvalidation-auc:0.71637\n",
      "[130]\tvalidation-aucpr:0.33339\tvalidation-auc:0.71641\n",
      "[131]\tvalidation-aucpr:0.33422\tvalidation-auc:0.71672\n",
      "[132]\tvalidation-aucpr:0.33434\tvalidation-auc:0.71681\n",
      "[133]\tvalidation-aucpr:0.33439\tvalidation-auc:0.71684\n",
      "[134]\tvalidation-aucpr:0.33450\tvalidation-auc:0.71689\n",
      "[135]\tvalidation-aucpr:0.33450\tvalidation-auc:0.71689\n",
      "[136]\tvalidation-aucpr:0.33450\tvalidation-auc:0.71689\n",
      "[137]\tvalidation-aucpr:0.33580\tvalidation-auc:0.71740\n",
      "[138]\tvalidation-aucpr:0.33570\tvalidation-auc:0.71734\n",
      "[139]\tvalidation-aucpr:0.33583\tvalidation-auc:0.71739\n",
      "[140]\tvalidation-aucpr:0.33592\tvalidation-auc:0.71745\n",
      "[141]\tvalidation-aucpr:0.33653\tvalidation-auc:0.71773\n",
      "[142]\tvalidation-aucpr:0.33670\tvalidation-auc:0.71782\n",
      "[143]\tvalidation-aucpr:0.33670\tvalidation-auc:0.71781\n",
      "[144]\tvalidation-aucpr:0.33671\tvalidation-auc:0.71782\n",
      "[145]\tvalidation-aucpr:0.33667\tvalidation-auc:0.71782\n",
      "[146]\tvalidation-aucpr:0.33695\tvalidation-auc:0.71794\n",
      "[147]\tvalidation-aucpr:0.33704\tvalidation-auc:0.71806\n",
      "[148]\tvalidation-aucpr:0.33704\tvalidation-auc:0.71801\n",
      "[149]\tvalidation-aucpr:0.33702\tvalidation-auc:0.71801\n",
      "[150]\tvalidation-aucpr:0.33702\tvalidation-auc:0.71801\n",
      "[151]\tvalidation-aucpr:0.33736\tvalidation-auc:0.71813\n",
      "[152]\tvalidation-aucpr:0.33736\tvalidation-auc:0.71812\n",
      "[153]\tvalidation-aucpr:0.33734\tvalidation-auc:0.71814\n",
      "[154]\tvalidation-aucpr:0.33755\tvalidation-auc:0.71833\n",
      "[155]\tvalidation-aucpr:0.33755\tvalidation-auc:0.71833\n",
      "[156]\tvalidation-aucpr:0.33770\tvalidation-auc:0.71842\n",
      "[157]\tvalidation-aucpr:0.33770\tvalidation-auc:0.71842\n",
      "[158]\tvalidation-aucpr:0.33770\tvalidation-auc:0.71842\n",
      "[159]\tvalidation-aucpr:0.33782\tvalidation-auc:0.71849\n",
      "[160]\tvalidation-aucpr:0.33784\tvalidation-auc:0.71847\n",
      "[161]\tvalidation-aucpr:0.33788\tvalidation-auc:0.71851\n",
      "[162]\tvalidation-aucpr:0.33788\tvalidation-auc:0.71851\n",
      "[163]\tvalidation-aucpr:0.33783\tvalidation-auc:0.71847\n",
      "[164]\tvalidation-aucpr:0.33783\tvalidation-auc:0.71846\n",
      "[165]\tvalidation-aucpr:0.33784\tvalidation-auc:0.71846\n",
      "[166]\tvalidation-aucpr:0.33784\tvalidation-auc:0.71846\n",
      "[167]\tvalidation-aucpr:0.33788\tvalidation-auc:0.71849\n",
      "[168]\tvalidation-aucpr:0.33794\tvalidation-auc:0.71851\n",
      "[169]\tvalidation-aucpr:0.33795\tvalidation-auc:0.71851\n",
      "[170]\tvalidation-aucpr:0.33823\tvalidation-auc:0.71861\n",
      "[171]\tvalidation-aucpr:0.33821\tvalidation-auc:0.71859\n",
      "[172]\tvalidation-aucpr:0.33830\tvalidation-auc:0.71865\n",
      "[173]\tvalidation-aucpr:0.33844\tvalidation-auc:0.71871\n",
      "[174]\tvalidation-aucpr:0.33844\tvalidation-auc:0.71871\n",
      "[175]\tvalidation-aucpr:0.33848\tvalidation-auc:0.71874\n",
      "[176]\tvalidation-aucpr:0.33848\tvalidation-auc:0.71874\n",
      "[177]\tvalidation-aucpr:0.33852\tvalidation-auc:0.71870\n",
      "[178]\tvalidation-aucpr:0.33858\tvalidation-auc:0.71868\n",
      "[179]\tvalidation-aucpr:0.33856\tvalidation-auc:0.71868\n",
      "[180]\tvalidation-aucpr:0.33901\tvalidation-auc:0.71889\n",
      "[181]\tvalidation-aucpr:0.33901\tvalidation-auc:0.71889\n",
      "[182]\tvalidation-aucpr:0.33939\tvalidation-auc:0.71910\n",
      "[183]\tvalidation-aucpr:0.33942\tvalidation-auc:0.71910\n",
      "[184]\tvalidation-aucpr:0.33954\tvalidation-auc:0.71915\n",
      "[185]\tvalidation-aucpr:0.33950\tvalidation-auc:0.71912\n",
      "[186]\tvalidation-aucpr:0.33973\tvalidation-auc:0.71921\n",
      "[187]\tvalidation-aucpr:0.33973\tvalidation-auc:0.71921\n",
      "[188]\tvalidation-aucpr:0.33977\tvalidation-auc:0.71920\n",
      "[189]\tvalidation-aucpr:0.33978\tvalidation-auc:0.71920\n",
      "[190]\tvalidation-aucpr:0.33995\tvalidation-auc:0.71934\n",
      "[191]\tvalidation-aucpr:0.33997\tvalidation-auc:0.71933\n",
      "[192]\tvalidation-aucpr:0.33997\tvalidation-auc:0.71933\n",
      "[193]\tvalidation-aucpr:0.33997\tvalidation-auc:0.71933\n",
      "[194]\tvalidation-aucpr:0.33999\tvalidation-auc:0.71934\n",
      "[195]\tvalidation-aucpr:0.33999\tvalidation-auc:0.71934\n",
      "[196]\tvalidation-aucpr:0.33999\tvalidation-auc:0.71936\n",
      "[197]\tvalidation-aucpr:0.33999\tvalidation-auc:0.71936\n",
      "[198]\tvalidation-aucpr:0.33998\tvalidation-auc:0.71933\n",
      "[199]\tvalidation-aucpr:0.33998\tvalidation-auc:0.71933\n",
      "[200]\tvalidation-aucpr:0.33998\tvalidation-auc:0.71933\n",
      "[201]\tvalidation-aucpr:0.33998\tvalidation-auc:0.71933\n",
      "[202]\tvalidation-aucpr:0.33998\tvalidation-auc:0.71933\n",
      "[203]\tvalidation-aucpr:0.34000\tvalidation-auc:0.71936\n",
      "[204]\tvalidation-aucpr:0.34001\tvalidation-auc:0.71936\n",
      "[205]\tvalidation-aucpr:0.34001\tvalidation-auc:0.71936\n",
      "[206]\tvalidation-aucpr:0.34001\tvalidation-auc:0.71936\n",
      "[207]\tvalidation-aucpr:0.34001\tvalidation-auc:0.71936\n",
      "[208]\tvalidation-aucpr:0.34002\tvalidation-auc:0.71938\n",
      "[209]\tvalidation-aucpr:0.34001\tvalidation-auc:0.71937\n",
      "[210]\tvalidation-aucpr:0.33999\tvalidation-auc:0.71939\n",
      "[211]\tvalidation-aucpr:0.34006\tvalidation-auc:0.71942\n",
      "[212]\tvalidation-aucpr:0.34006\tvalidation-auc:0.71942\n",
      "[213]\tvalidation-aucpr:0.34006\tvalidation-auc:0.71942\n",
      "[214]\tvalidation-aucpr:0.34006\tvalidation-auc:0.71942\n",
      "[215]\tvalidation-aucpr:0.34006\tvalidation-auc:0.71941\n",
      "[216]\tvalidation-aucpr:0.34007\tvalidation-auc:0.71942\n",
      "[217]\tvalidation-aucpr:0.34007\tvalidation-auc:0.71942\n",
      "[218]\tvalidation-aucpr:0.34007\tvalidation-auc:0.71942\n",
      "[219]\tvalidation-aucpr:0.34028\tvalidation-auc:0.71969\n",
      "[220]\tvalidation-aucpr:0.34027\tvalidation-auc:0.71967\n",
      "[221]\tvalidation-aucpr:0.34027\tvalidation-auc:0.71967\n",
      "[222]\tvalidation-aucpr:0.34027\tvalidation-auc:0.71967\n",
      "[223]\tvalidation-aucpr:0.34027\tvalidation-auc:0.71967\n",
      "[224]\tvalidation-aucpr:0.34010\tvalidation-auc:0.71945\n",
      "[225]\tvalidation-aucpr:0.34012\tvalidation-auc:0.71952\n",
      "[226]\tvalidation-aucpr:0.34012\tvalidation-auc:0.71952\n",
      "[227]\tvalidation-aucpr:0.34011\tvalidation-auc:0.71951\n",
      "[228]\tvalidation-aucpr:0.34014\tvalidation-auc:0.71954\n",
      "[229]\tvalidation-aucpr:0.34017\tvalidation-auc:0.71954\n",
      "[230]\tvalidation-aucpr:0.34015\tvalidation-auc:0.71956\n",
      "[231]\tvalidation-aucpr:0.34015\tvalidation-auc:0.71956\n",
      "[232]\tvalidation-aucpr:0.34015\tvalidation-auc:0.71956\n",
      "[233]\tvalidation-aucpr:0.34015\tvalidation-auc:0.71956\n",
      "[234]\tvalidation-aucpr:0.34015\tvalidation-auc:0.71956\n",
      "[235]\tvalidation-aucpr:0.34015\tvalidation-auc:0.71956\n",
      "[236]\tvalidation-aucpr:0.34015\tvalidation-auc:0.71956\n",
      "[237]\tvalidation-aucpr:0.34022\tvalidation-auc:0.71959\n",
      "[238]\tvalidation-aucpr:0.34024\tvalidation-auc:0.71960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/06 07:51:46 WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.\n",
      "2025/01/06 07:51:46 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: [07:51:46] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0750514818a16474a-1\\xgboost\\xgboost-ci-windows\\src\\tree\\tree_model.cc:899: Check failed: !HasCategoricalSplit(): Please use JSON/UBJSON for saving models with categorical splits.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento e logging concluídos.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, balanced_accuracy_score, average_precision_score,\n",
    "    log_loss, brier_score_loss, cohen_kappa_score, matthews_corrcoef, roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "mlflow.xgboost.autolog()\n",
    "with mlflow.start_run(\n",
    "    experiment_id=experiment_id,\n",
    "    run_name='Treinamento e avaliação XGBoost',\n",
    "    description='Treinamento com melhores hiperparâmetros e avaliação do modelo final',\n",
    "    tags={\"Tipo\": \"Classificação\", \"Modelo\": \"XGBoost\", \"Etapa\": \"Treinamento final\"}):\n",
    "    \n",
    "    \n",
    "    # Log dos parâmetros do modelo\n",
    "    mlflow.log_params(best_params)\n",
    "    \n",
    "    num_boost_round = int(best_hyperparams['n_estimators'])\n",
    "    # Treinamento do modelo\n",
    "    model_class = xgb.train(\n",
    "        params=best_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        evals=[(dtest_valid, 'validation')],\n",
    "        early_stopping_rounds=20,\n",
    "        verbose_eval=True)\n",
    "    \n",
    "    # Previsões\n",
    "    y_pred_proba = model_class.predict(dtest)\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "    \n",
    "    # Métricas de desempenho\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred),\n",
    "        \"recall\": recall_score(y_test, y_pred),\n",
    "        \"f1_score\": f1_score(y_test, y_pred),\n",
    "        \"balanced_accuracy\": balanced_accuracy_score(y_test, y_pred),\n",
    "        \"auc\": roc_auc_score(y_test, y_pred_proba),\n",
    "        \"prauc\": average_precision_score(y_test, y_pred_proba),\n",
    "        \"mcc\": matthews_corrcoef(y_test, y_pred),\n",
    "        \"log_loss\": log_loss(y_test, y_pred_proba),\n",
    "        \"brier_score\": brier_score_loss(y_test, y_pred_proba),\n",
    "        \"cohen_kappa\": cohen_kappa_score(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "    # Log de métricas individualmente\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        mlflow.log_metric(metric_name, metric_value)\n",
    "    \n",
    "    # Gráficos e artefatos\n",
    "    # Matriz de Confusão\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\")\n",
    "    plt.title('Matriz de Confusão')\n",
    "    plt.xlabel('Predito')\n",
    "    plt.ylabel('Real')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    mlflow.log_artifact('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Importância das Features\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    xgb.plot_importance(model_class, max_num_features=20)\n",
    "    plt.title('Importância das Features')\n",
    "    plt.savefig('feature_importance.png')\n",
    "    mlflow.log_artifact('feature_importance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Curva ROC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, linestyle='--', label='Curva ROC (AUC = {:.3f})'.format(metrics[\"auc\"]))\n",
    "    plt.title('Curva ROC')\n",
    "    plt.xlabel('Taxa de Falsos Positivos')\n",
    "    plt.ylabel('Taxa de Verdadeiros Positivos')\n",
    "    plt.legend()\n",
    "    plt.savefig('roc_curve.png')\n",
    "    mlflow.log_artifact('roc_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Curva de Precisão-Recall\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall_vals, precision_vals, marker='.', label='PRAUC = {:.3f}'.format(metrics[\"prauc\"]))\n",
    "    plt.title('Curva de Precisão-Recall')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precisão')\n",
    "    plt.legend()\n",
    "    plt.savefig('precision_recall_curve.png')\n",
    "    mlflow.log_artifact('precision_recall_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "    # Logar o modelo no MLflow e obter o URI\n",
    "    model_uri = mlflow.xgboost.log_model(\n",
    "            xgb_model=model_class,\n",
    "            artifact_path=\"modelo_xgboost\",\n",
    "            model_format=\"json\"\n",
    "        )\n",
    "\n",
    "    print(\"Treinamento e logging concluídos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0a7060-10b1-4817-97d5-6c71d6ca156e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Alteernativa 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a704f4e-d38b-4f38-a962-a1c8fa4d9286",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(experiment_id=experiment_id, run_name='Pre-processamento',\n",
    "                      run_id='ba596ce0c0ff43228f85f3ef932a8310',\n",
    "                      nested=True,\n",
    "                      description = 'Garantir o input correto dos modelos',\n",
    "                      tags = {\"Pre-processamento\": \"preparação para treinamento\", \"objetivo\": \"garantir o input correto dos dados\", \"Versão da etapa\": \"1.0\"}):\n",
    "    \n",
    "   \n",
    "    \n",
    "    dft = df[chosen_columns].sample(frac=0.3, random_state=13)\n",
    "    \n",
    "    # Colunas que precisam passar por one hot encoding\n",
    "    list_dummies = ['nome_empresas','codigo_tipo_linha','descricao_origem','descricao_destino','pais_origem','pais_destino','continente_origem',\n",
    "                 'continente_destino','cidade_origem','cidade_destino','uf_origem','uf_destino','mes_partida',\n",
    "                 'dia_semana_chegada']\n",
    "\n",
    "    final_data = pd.DataFrame()\n",
    "    # Logar os parâmetros\n",
    "    mlflow.log_param(\"Colunas escolhidas\", chosen_columns)\n",
    "    mlflow.log_param(\"Index\", 'num_cpf')\n",
    "    mlflow.log_param(\"Colunas para one-hot encoding\", list_dummies)\n",
    "    \n",
    "    # Logar métricas\n",
    "    mlflow.log_metric(\"Quantidade de colunas\", len(chosen_columns))\n",
    "    mlflow.log_metric(\"Quantidade de colunas dummies\", len(list_dummies))\n",
    "    mlflow.log_metric(\"Quantidade de colunas não dummies\", len(chosen_columns) - len(list_dummies) - 1) \n",
    "    \n",
    "    ### One hot encoding\n",
    "    with mlflow.start_run(experiment_id=experiment_id, nested=True, run_name='One hot encoding', run_id='8836439277bc460e8767f9e6b7311883',\n",
    "                      description = 'Transformação das colunas categoricas em númericas',\n",
    "                      tags = {\"One hot encoding\": \"Transformar categorica em númerica\", \"objetivo\": \"garantir o input correto dos dados\", \"Versão da etapa\": \"1.0\"}):\n",
    "        for column in list_dummies:\n",
    "            encoder = preprocessing.OneHotEncoder(handle_unknown='ignore')\n",
    "            encoder.fit(dft[[column]])\n",
    "            \n",
    "            # Logar parâmetros para cada coluna processada\n",
    "            mlflow.log_param(f\"Coluna_{column.lower()}\", column.lower())\n",
    "            \n",
    "            enc_df = pd.DataFrame(encoder.transform(dft[[column]]).toarray(), \n",
    "                                  columns=encoder.get_feature_names_out([column]))\n",
    "            final_data = pd.concat([final_data, enc_df], axis=1)\n",
    "\n",
    "        final_data['status_do_voo'] = dft['status_do_voo'].values\n",
    "\n",
    "        dt_ax = final_data.drop(columns=[\"status_do_voo\"])\n",
    "        dt_ay = final_data[['status_do_voo']].copy()\n",
    "\n",
    "        # Transformação da coluna em valores binarios. Pontual = 1 e Atrasado = 0\n",
    "        label_encoder = LabelEncoder()\n",
    "        dt_ay_enc = label_encoder.fit_transform(dt_ay)\n",
    "        dt_ay_df = pd.DataFrame(dt_ay_enc, columns=dt_ay.columns)\n",
    "\n",
    "        # Suponha que 'df' é o seu DataFrame\n",
    "        column_names = dt_ax.columns.tolist()\n",
    "        name_map = clean_column_names(column_names)\n",
    "        \n",
    "        # Renomear colunas no DataFrame\n",
    "        dt_ax.rename(columns=name_map, inplace=True)\n",
    "        \n",
    "    ### Normalização / Segmentação  treino e teste / Smote\n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name='Normalização e Smote', nested=True,run_id='a058314be1ff4283b8fafd1168611eba',\n",
    "                      description = 'Implementação da etapa de normalização e SMOTE dos dados. Essas etapas são essenciais para evitar overfiting e underfitting',\n",
    "                      tags = {\"Normalização e SMOTE\": \"Normalização em range de 0 a 1 e criação de dados sinteticos para balencear\", \"objetivo\": \"garantir qualidade no correto dos dados\", \"Versão da etapa\": \"1.0\"}):\n",
    "        # Normalização dos dados\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(dt_ax)\n",
    "        X_scaled_df = pd.DataFrame(X_scaled, columns=dt_ax.columns)\n",
    "    \n",
    "        # Segmentação em Treino (85%) e Teste (15%)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, dt_ay_df, random_state=13, test_size=0.15)\n",
    "    \n",
    "        # Logar distribuição das classes antes do SMOTE\n",
    "        log_class_distribution(y_test, 'original')\n",
    "    \n",
    "        # Aplicar SMOTE\n",
    "        smote = SMOTE(random_state=13)\n",
    "        X_smote_a, y_smote_a = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        X_test = X_test.reset_index().drop(columns = 'index')\n",
    "        y_test = y_test.reset_index().drop(columns = 'index')\n",
    "    \n",
    "        # Logar distribuição das classes após SMOTE\n",
    "        log_class_distribution(y_smote_a, 'SMOTE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca24da70-4c24-483d-b232-33ce8df2ed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unified_hyper_tuning(space):\n",
    "    \"\"\"\n",
    "    Realiza o ajuste de hiperparâmetros e treinamento de um modelo XGBoost com logging completo utilizando MLflow.\n",
    "    \n",
    "    Args:\n",
    "        space (dict): Dicionário contendo os hiperparâmetros para o modelo XGBoost.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dicionário contendo o 'loss' (negativo da média do AUC) e o 'status'.\n",
    "    \"\"\"\n",
    "    mlflow.xgboost.autolog()\n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name='Unified Model Training and Tuning', nested=True):\n",
    "        #  Configuração do modelo com os parâmetros do espaço\n",
    "        clf = xgb.XGBClassifier(max_depth = space['max_depth'],\n",
    "                                  learning_rate = space['learning_rate'],\n",
    "                                  reg_alpha = space['reg_alpha'],\n",
    "                                  reg_lambda = space['reg_lambda'],\n",
    "                                  min_child_weight = space['min_child_weight'],\n",
    "                                  subsample = space['subsample'],\n",
    "                                  colsample_bytree = space['colsample_bytree'],\n",
    "                                  gamma = space['gamma'],\n",
    "                                  objective = space['objective'],\n",
    "                                  seed = space['seed'])\n",
    "        \n",
    "        # StratifiedKFold para manter a proporção de classes em cada fold\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Avaliação usando cross_val_score no conjunto de treinamento\n",
    "        auc_scores = cross_val_score(clf, X_train, y_train, cv=skf, scoring='roc_auc')\n",
    "        mean_auc = auc_scores.mean()\n",
    "\n",
    "        # Logando a média do AUC\n",
    "        mlflow.log_metric('mean_auc', mean_auc)\n",
    "        \n",
    "        model = clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Teste do modelo\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "        # Teste do modelo e log das curvas\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    \n",
    "        # Plotar e salvar a Curva de Precisão-Recall\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(recall, precision, marker='.')\n",
    "        plt.title('Curva de Precisão-Recall')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precisão')\n",
    "        plt.savefig('precision_recall_curve.png')\n",
    "        plt.close()\n",
    "    \n",
    "        # Plotar e salvar a Curva ROC\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, linestyle='--')\n",
    "        plt.title('Curva ROC')\n",
    "        plt.xlabel('Taxa de Falso Positivo')\n",
    "        plt.ylabel('Taxa de Verdadeiro Positivo')\n",
    "        plt.savefig('roc_curve.png')\n",
    "        plt.close()\n",
    "    \n",
    "        # Logar gráficos como artefatos\n",
    "        mlflow.log_artifact('precision_recall_curve.png')\n",
    "        mlflow.log_artifact('roc_curve.png')\n",
    "\n",
    "        # Create a model signature\n",
    "        signature = infer_signature(X_test, model.predict(X_test))\n",
    "        model_info = mlflow.xgboost.log_model(model, \"modelo_xgboost\", signature=signature) \n",
    "        \n",
    "        mlflow.xgboost.log_model(model, \"model_xgb\", signature=signature)\n",
    "        model_uri = mlflow.get_artifact_uri(\"model_xgb\")\n",
    "        \n",
    "        eval_data = pd.DataFrame(X_test, columns=dt_ax.columns)\n",
    "        eval_data['atraso30_m3'] = y_test.reset_index(drop=True)\n",
    "        \n",
    "        result = mlflow.evaluate(model_uri,\n",
    "                                 eval_data,\n",
    "                                 targets=\"atraso30_m3\",\n",
    "                                 model_type=\"classifier\",\n",
    "                                 evaluators=[\"default\"])\n",
    "\n",
    "        # A função de perda é o negativo da média do AUC para otimização\n",
    "        return {'loss': -mean_auc, 'status': STATUS_OK}\n",
    "\n",
    "space = {\n",
    "  'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n",
    "  'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "  'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n",
    "  'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n",
    "  'gamma': hp.quniform('gamma', 0.5, 1, 0.05),\n",
    "  'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "  'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "  'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),\n",
    "  'objective': 'binary:logistic',\n",
    "  'seed': 123, # Set a seed for deterministic training\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866616d-12d0-45fe-bf8c-ccbd16a1ce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etapa de hipertunning\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name='Hipertunnig', nested=True,\n",
    "                      description = 'Busca pelos melhores parametros. Os modelos testados são armazenados, mesmo que não tenha os melhores parametros.',\n",
    "                      tags = {\"Hipertunnig\": \"Melhores parametros\", \"objetivo\": \"garantir os melhores parametros para o modelo\", \"Versão da etapa\": \"1.0\"}):\n",
    "    # Executando a otimização\n",
    "    trials = Trials()\n",
    "    best_hyperparams = fmin(fn=unified_hyper_tuning, \n",
    "                            space=space, \n",
    "                            algo=tpe.suggest, \n",
    "                            max_evals=5, \n",
    "                            trials=trials)\n",
    "    \n",
    "    # Obtendo os melhores hiperparâmetros\n",
    "    mlflow.log_params(best_hyperparams)\n",
    "    best_hyperparams = space_eval(space, best_hyperparams)\n",
    "    print(\"Melhores hiperparâmetros:\", best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f5c8de-3ef0-46b3-9208-d89d4a27aa75",
   "metadata": {},
   "source": [
    "# Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5da9b1-1c67-442a-8e71-41f4e8a384c1",
   "metadata": {},
   "source": [
    "## Criando ou carregando o experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7bfa196-9c02-495a-ad44-5e74002f2e3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O experimento 'Teste CatBoost MLflow Aviação' já existe.\n"
     ]
    }
   ],
   "source": [
    "# Nome do experimento que você deseja verificar/criar\n",
    "experiment_name = \"Teste CatBoost MLflow Aviação\"\n",
    "\n",
    "# Verificar se o experimento já existe\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "# Se o experimento não existir, cria-o\n",
    "if experiment is None:\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    print(f\"O experimento '{experiment_name}' foi criado.\")\n",
    "else:\n",
    "    print(f\"O experimento '{experiment_name}' já existe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b608913-c200-4f80-aeee-b8c93bbe5605",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O experimento id é:'184784723568013405'\n"
     ]
    }
   ],
   "source": [
    "# Verificar se o experimento já existe\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "# Id do experimento\n",
    "experiment_id = experiment.experiment_id\n",
    "print(f\"O experimento id é:'{experiment_id}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce16e347-23e3-4a19-8c1b-6c041a533bd4",
   "metadata": {},
   "source": [
    "## Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9533b7e6-e30a-4356-9b04-fc11acf22af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(experiment_id=experiment_id,\n",
    "    run_name=\"Pipeline de Pré-processamento CatBoost\", \n",
    "                      description=\"Pipeline completo para preparação de dados históricos de voos\",\n",
    "                      tags={\"Etapa\": \"Pipeline de Pre-processamento\", \"versão\": \"1.0\"}):\n",
    "\n",
    "    # Etapa 1: Carregamento dos dados tratados\n",
    "    with mlflow.start_run(experiment_id=experiment_id,run_name=\"Carregando dataset tratado\", nested=True):\n",
    "        # Lendo os dados\n",
    "        file_path = 'df_treinamento_oos.csv'\n",
    "        df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Etapa 2: Exclusão de colunas desnecessárias\n",
    "    with mlflow.start_run(experiment_id=experiment_id,run_name=\"Exclusão de Colunas\", nested=True):\n",
    "        df = df.drop(columns=['codigo_di', 'codigo_tipo_linha'])\n",
    "        mlflow.log_param(\"colunas_excluidas\", ['codigo_di', 'codigo_tipo_linha'])\n",
    "\n",
    "    # Etapa 3: Identificação de colunas categóricas\n",
    "    with mlflow.start_run(experiment_id=experiment_id,run_name=\"Identificação de Categóricas\", nested=True):\n",
    "        list_dummies = df.drop(columns='status_do_voo').select_dtypes(include=['object']).columns.tolist()\n",
    "        mlflow.log_param(\"colunas_categoricas\", list_dummies)\n",
    "\n",
    "    # Etapa 4: Seleção de features e variável-alvo\n",
    "    with mlflow.start_run(experiment_id=experiment_id,run_name=\"Seleção de Features\", nested=True):\n",
    "        dt_ax = df.drop(columns=[\"status_do_voo\"])\n",
    "        dt_ay = df['status_do_voo'].map({'Pontual': 0, 'Atrasado': 1})\n",
    "        mlflow.log_param(\"target_mapping\", {'Pontual': 0, 'Atrasado': 1})\n",
    "        mlflow.log_param(\"n_features\", dt_ax.shape[1])\n",
    "\n",
    "    # Etapa 5: Codificação de colunas categóricas\n",
    "    with mlflow.start_run(experiment_id=experiment_id,run_name=\"Codificação de Categóricas\", nested=True):\n",
    "        label_encoders = {}\n",
    "        for col in list_dummies:\n",
    "            le = LabelEncoder()\n",
    "            dt_ax[col] = le.fit_transform(dt_ax[col])\n",
    "            label_encoders[col] = le\n",
    "        mlflow.log_param(\"n_label_encoded_columns\", len(list_dummies))\n",
    "\n",
    "    # Etapa 6: Segmentação em treino, teste e validação\n",
    "    with mlflow.start_run(experiment_id=experiment_id,run_name=\"Segmentação dos Dados\", nested=True):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(dt_ax, dt_ay, random_state=33, test_size=0.142)\n",
    "        X_train_valid, X_test_valid, y_train_valid, y_test_valid = train_test_split(X_train, y_train, random_state=33, test_size=0.165)\n",
    "\n",
    "        mlflow.log_param(\"train_size\", len(X_train_valid))\n",
    "        mlflow.log_param(\"validation_size\", len(X_test_valid))\n",
    "        mlflow.log_param(\"test_size\", len(X_test))\n",
    "\n",
    "    # Etapa 7: Reversão e finalização das colunas categóricas\n",
    "    with mlflow.start_run(experiment_id=experiment_id,run_name=\"Reversão de Colunas Categóricas\", nested=True):\n",
    "        def revert_to_category(data, label_encoders, list_dummies):\n",
    "            for col in list_dummies:\n",
    "                if col in data.columns:\n",
    "                    le = label_encoders[col]\n",
    "                    data[col] = le.inverse_transform(data[col])\n",
    "            return data\n",
    "\n",
    "        X_train_valid = revert_to_category(X_train_valid, label_encoders, list_dummies)\n",
    "        X_test_valid = revert_to_category(X_test_valid, label_encoders, list_dummies)\n",
    "        X_test = revert_to_category(X_test, label_encoders, list_dummies)\n",
    "\n",
    "        mlflow.log_param(\"categorical_columns_finalized\", list_dummies)\n",
    "\n",
    "    # Etapa 7: Resumo do Pipeline\n",
    "    mlflow.log_param(\"pipeline_status\", \"Concluído\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f36bbe5-ebb4-4a72-967f-98a4e1cb1da2",
   "metadata": {},
   "source": [
    "## Treinamento sem hipertuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4504182e-8a2f-44cb-9b40-f4e55baad229",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Desenvolvimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193b4fba-27c1-4ffb-a937-5bdb0e79433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with mlflow.start_run(\n",
    "    experiment_id=experiment_id,\n",
    "    run_name='Treinamento e avaliação CatBoost',\n",
    "    nested=True,\n",
    "    description='Treinamento com melhores hiperparâmetros e avaliação do modelo final',\n",
    "    tags={\"Tipo\": \"Classificação\", \"Modelo\": \"CatBoost\", \"Etapa\": \"Treinamento final\"}):\n",
    "    \n",
    "    # Log dos parâmetros do modelo\n",
    "    mlflow.log_params(best_params)\n",
    "    \n",
    "    # Configuração do modelo CatBoostClassifier\n",
    "    classifier_params = best_params.copy()\n",
    "    cat_features = list(X_train_valid.select_dtypes(include=['object']))\n",
    "    model = CatBoostClassifier(\n",
    "        cat_features=cat_features, \n",
    "        eval_metric='AUC')\n",
    "\n",
    "    \n",
    "    # Treinamento do modelo\n",
    "    model.fit(X_train_valid, y_train_valid, \n",
    "              eval_set=(X_test_valid, y_test_valid), \n",
    "              cat_features=cat_features, \n",
    "              verbose=100,\n",
    "              plot=True)\n",
    "    \n",
    "    # Previsões\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "    \n",
    "     # Métricas de desempenho\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    fpr = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "    fnr = FN / (FN + TP) if (FN + TP) > 0 else 0\n",
    "    g_mean = np.sqrt(sensitivity * specificity)\n",
    "    \n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred),\n",
    "        \"recall (sensibilidade)\": recall_score(y_test, y_pred),\n",
    "        \"f1_score\": f1_score(y_test, y_pred),\n",
    "        \"balanced_accuracy\": balanced_accuracy_score(y_test, y_pred),\n",
    "        \"specificity\": specificity,\n",
    "        \"auc\": roc_auc_score(y_test, y_pred_proba),\n",
    "        \"prauc\": average_precision_score(y_test, y_pred_proba),\n",
    "        \"mcc\": matthews_corrcoef(y_test, y_pred),\n",
    "        \"log_loss\": log_loss(y_test, y_pred_proba),\n",
    "        \"brier_score\": brier_score_loss(y_test, y_pred_proba),\n",
    "        \"cohen_kappa\": cohen_kappa_score(y_test, y_pred),\n",
    "        \"false_positive_rate (FPR)\": fpr,\n",
    "        \"false_negative_rate (FNR)\": fnr,\n",
    "        \"geometric_mean (G-Mean)\": g_mean\n",
    "    }\n",
    "\n",
    "\n",
    "    # Log de métricas individualmente\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        mlflow.log_metric(metric_name, metric_value)\n",
    "    \n",
    "    # Gráficos e artefatos\n",
    "    # Matriz de Confusão\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\")\n",
    "    plt.title('Matriz de Confusão')\n",
    "    plt.xlabel('Predito')\n",
    "    plt.ylabel('Real')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    mlflow.log_artifact('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Curva ROC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, linestyle='--', label='Curva ROC (AUC = {:.3f})'.format(metrics[\"auc\"]))\n",
    "    plt.title('Curva ROC')\n",
    "    plt.xlabel('Taxa de Falsos Positivos')\n",
    "    plt.ylabel('Taxa de Verdadeiros Positivos')\n",
    "    plt.legend()\n",
    "    plt.savefig('roc_curve.png')\n",
    "    mlflow.log_artifact('roc_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Curva de Precisão-Recall\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall_vals, precision_vals, marker='.', label='PRAUC = {:.3f}'.format(metrics[\"prauc\"]))\n",
    "    plt.title('Curva de Precisão-Recall')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precisão')\n",
    "    plt.legend()\n",
    "    plt.savefig('precision_recall_curve.png')\n",
    "    mlflow.log_artifact('precision_recall_curve.png')\n",
    "    plt.close()\n",
    "\n",
    "    # SHAP Importance\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(X_test)\n",
    "    shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "    sorted_idx = shap_importance.argsort()\n",
    "\n",
    "    # Gráfico de importância SHAP\n",
    "    fig = plt.figure(figsize=(7, 7))\n",
    "    plt.barh(range(len(sorted_idx)), shap_importance[sorted_idx], align='center')\n",
    "    plt.yticks(range(len(sorted_idx)), np.array(X_test.columns)[sorted_idx])\n",
    "    plt.title('SHAP Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('shap_importance.png')\n",
    "    mlflow.log_artifact('shap_importance.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Beeswarm SHAP\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.plots.beeswarm(shap_values, max_display=15, show=False)\n",
    "    plt.title('SHAP Beeswarm')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('shap_beeswarm.png')\n",
    "    mlflow.log_artifact('shap_beeswarm.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Registrar o modelo no MLflow\n",
    "    signature = infer_signature(X_test, y_pred_proba)\n",
    "    mlflow.catboost.log_model(\n",
    "        model,\n",
    "        artifact_path=\"model_catboost\",\n",
    "        signature=signature\n",
    "    )\n",
    "    \n",
    "    print(\"Treinamento, logging e gráfico SHAP concluídos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dde854e-0931-4eab-a401-aaa40aa4022f",
   "metadata": {},
   "source": [
    "## Hipertuning Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52f72f-fdba-4cb1-a841-6eef9f6ad2da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Hipertuning desenvolvimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323ee2a5-440c-4099-8773-ff33e9018a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "space_catboost = {\n",
    "    'iterations': scope.int(hp.quniform('iterations', 100, 1000, 50)),   # Número de árvores\n",
    "    'learning_rate': hp.loguniform('learning_rate', -3, -0.3),           # Taxa de aprendizado (0.05 ~ 0.7)\n",
    "    'depth': scope.int(hp.quniform('depth', 4, 12, 1)),                  # Profundidade da árvore (controle de overfitting)\n",
    "    'l2_leaf_reg': hp.loguniform('l2_leaf_reg', -3, 2),                  # Regularização L2 (1 ~ 100)\n",
    "    'random_strength': hp.uniform('random_strength', 0, 2),              # Aleatoriedade nas divisões\n",
    "    'bagging_temperature': hp.uniform('bagging_temperature', 0, 1),      # Temperatura para amostragem de dados\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 4, 8),            # Peso para classes desbalanceadas\n",
    "    'min_data_in_leaf': scope.int(hp.quniform('min_data_in_leaf', 10, 100, 10)),  # Mínimo de dados por folha\n",
    "    'max_bin': scope.int(hp.quniform('max_bin', 128, 256, 32)),          # Número máximo de bins\n",
    "    'grow_policy': hp.choice('grow_policy', ['Depthwise', 'Lossguide']), # Política de crescimento\n",
    "    'eval_metric': 'AUC',                                                # Métrica de avaliação\n",
    "    'task_type': 'GPU',                                                  # Utilizar GPU\n",
    "    'random_seed': 42                                                    # Reprodutibilidade\n",
    "}\n",
    "\n",
    "\n",
    "# Função objetivo para o Hyperopt\n",
    "def objective(params):\n",
    "    params['eval_metric'] = params['eval_metric']  # Define a métrica de avaliação\n",
    "    params['loss_function'] = 'Logloss'           # Objetivo de classificação binária\n",
    "    params['verbose'] = False                         # Reduz a verbosidade do treinamento\n",
    "\n",
    "    # Inicialização do modelo\n",
    "    model = CatBoostClassifier(**params,cat_features=cat_features, )\n",
    "    \n",
    "    # Treinamento\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=(X_test, y_test),\n",
    "        early_stopping_rounds=50,\n",
    "        cat_features=cat_features, \n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Predições e cálculo da métrica\n",
    "    preds = model.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, preds)\n",
    "    \n",
    "    # Retorna a métrica negativa\n",
    "    return {'loss': -auc, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "\n",
    "# Inicialização do Hyperopt\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    fn=objective,                     # Função objetivo\n",
    "    space=space_catboost,             # Espaço de busca\n",
    "    algo=tpe.suggest,                 # Algoritmo de busca (TPE)\n",
    "    max_evals=2,                     # Número de avaliações\n",
    "    trials=trials,                    # Armazena os resultados\n",
    "    rstate=np.random.default_rng(42)  # Reprodutibilidade\n",
    ")\n",
    "\n",
    "# Exibição dos melhores parâmetros\n",
    "print(\"Melhores parâmetros:\", best)\n",
    "\n",
    "\n",
    "# Ajuste dos Melhores Parâmetros\n",
    "best_params = {\n",
    "        'depth': int(best['depth']),  # Corrigido para \"depth\"\n",
    "        'random_strength': best['random_strength'],\n",
    "        'l2_leaf_reg': best['l2_leaf_reg'],\n",
    "        'bagging_temperature': best['bagging_temperature'],\n",
    "        'min_data_in_leaf': int(best['min_data_in_leaf']),  # Corrigido para \"min_data_in_leaf\"\n",
    "        'learning_rate': best['learning_rate'],\n",
    "        'iterations': int(best['iterations']),  # Corrigido para \"iterations\"\n",
    "        'scale_pos_weight': best['scale_pos_weight'],\n",
    "        'max_bin': int(best['max_bin']),\n",
    "        'grow_policy': ['Depthwise', 'Lossguide'][best['grow_policy']],  # Mapeia o índice para a string correta\n",
    "        'task_type': 'GPU',\n",
    "        'eval_metric': 'AUC',\n",
    "        'loss_function': 'Logloss',\n",
    "        'random_seed': 42,\n",
    "        'verbose': False }\n",
    "    \n",
    "\n",
    "# Treinamento do Modelo Final\n",
    "final_model = CatBoostClassifier(**best_params, cat_features=cat_features)\n",
    "    \n",
    "final_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=(X_test, y_test),\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=10,\n",
    "    plot=True)\n",
    "    \n",
    "# Avaliação do Modelo Final\n",
    "final_preds = final_model.predict_proba(X_test)[:, 1]\n",
    "final_auc = roc_auc_score(y_test, final_preds)\n",
    "print(f\"AUC do modelo final: {final_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac7ea9a-b429-4a7e-b169-8af27f16fa60",
   "metadata": {},
   "source": [
    "### Hipertuning Mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a96751c9-5a6c-4060-8155-a3cadca96eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_mlflow(params):\n",
    "    \"\"\"\n",
    "    Realiza o ajuste de hiperparâmetros e treinamento de um modelo CatBoost com logging completo utilizando MLflow.\n",
    "    \n",
    "    Args:\n",
    "        params (dict): Dicionário contendo os hiperparâmetros para o modelo CatBoost.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dicionário contendo o 'loss' (negativo da média do AUC) e o 'status'.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    cat_features = list(X_train_valid.select_dtypes(include=['object']))\n",
    "    \n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name='CatBoost Training and Tuning', nested=True,\n",
    "                         tags = {\"Hipertunnig\": \"Catboost\"}):\n",
    "        mlflow.log_params(params)\n",
    "        # Ajuste dos hiperparâmetros\n",
    "        params['loss_function'] = 'Logloss'           # Objetivo de classificação binária\n",
    "        params['verbose'] = False                         # Reduz a verbosidade do treinamento\n",
    "\n",
    "        # Inicialização do modelo\n",
    "        model = CatBoostClassifier(\n",
    "            **params,\n",
    "            cat_features=cat_features,\n",
    "        )\n",
    "        \n",
    "        # Treinamento\n",
    "        model.fit(\n",
    "            X_train_valid, y_train_valid,\n",
    "            eval_set=(X_test_valid, y_test_valid),\n",
    "            early_stopping_rounds=40,\n",
    "            cat_features=cat_features,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        # Previsões\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "        \n",
    "        # Métricas de desempenho\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        TN, FP, FN, TP = cm.ravel()\n",
    "        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        fpr = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "        fnr = FN / (FN + TP) if (FN + TP) > 0 else 0\n",
    "        g_mean = np.sqrt(sensitivity * specificity)\n",
    "\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred),\n",
    "            \"recall_sensibility\": recall_score(y_test, y_pred),  # Nome ajustado\n",
    "            \"f1_score\": f1_score(y_test, y_pred),\n",
    "            \"balanced_accuracy\": balanced_accuracy_score(y_test, y_pred),\n",
    "            \"specificity\": specificity,\n",
    "            \"auc\": roc_auc_score(y_test, y_pred_proba),\n",
    "            \"prauc\": average_precision_score(y_test, y_pred_proba),\n",
    "            \"mcc\": matthews_corrcoef(y_test, y_pred),\n",
    "            \"log_loss\": log_loss(y_test, y_pred_proba),\n",
    "            \"brier_score\": brier_score_loss(y_test, y_pred_proba),\n",
    "            \"cohen_kappa\": cohen_kappa_score(y_test, y_pred),\n",
    "            \"false_positive_rate_FPR\": fpr,  # Nome ajustado\n",
    "            \"false_negative_rate_FNR\": fnr,  # Nome ajustado\n",
    "            \"geometric_mean_GMean\": g_mean   # Nome ajustado\n",
    "        }\n",
    "\n",
    "        # Log de métricas\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "\n",
    "        # Gráficos e artefatos\n",
    "        output_dir = \"mlflow_artifacts\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Matriz de Confusão\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\")\n",
    "        plt.title('Matriz de Confusão')\n",
    "        plt.xlabel('Predito')\n",
    "        plt.ylabel('Real')\n",
    "        confusion_matrix_path = os.path.join(output_dir, \"confusion_matrix.png\")\n",
    "        plt.savefig(confusion_matrix_path)\n",
    "        mlflow.log_artifact(confusion_matrix_path)\n",
    "        plt.close()\n",
    "\n",
    "        # Curva ROC\n",
    "        fpr_vals, tpr_vals, _ = roc_curve(y_test, y_pred_proba)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr_vals, tpr_vals, linestyle='--', label='Curva ROC (AUC = {:.3f})'.format(metrics[\"auc\"]))\n",
    "        plt.title('Curva ROC')\n",
    "        plt.xlabel('Taxa de Falsos Positivos')\n",
    "        plt.ylabel('Taxa de Verdadeiros Positivos')\n",
    "        plt.legend()\n",
    "        roc_curve_path = os.path.join(output_dir, \"roc_curve.png\")\n",
    "        plt.savefig(roc_curve_path)\n",
    "        mlflow.log_artifact(roc_curve_path)\n",
    "        plt.close()\n",
    "\n",
    "        # Curva de Precisão-Recall\n",
    "        precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(recall_vals, precision_vals, marker='.', label='PRAUC = {:.3f}'.format(metrics[\"prauc\"]))\n",
    "        plt.title('Curva de Precisão-Recall')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precisão')\n",
    "        plt.legend()\n",
    "        pr_curve_path = os.path.join(output_dir, \"precision_recall_curve.png\")\n",
    "        plt.savefig(pr_curve_path)\n",
    "        mlflow.log_artifact(pr_curve_path)\n",
    "        plt.close()\n",
    "\n",
    "        # SHAP Importance\n",
    "        explainer = shap.Explainer(model)\n",
    "        shap_values = explainer(X_test)\n",
    "        shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "        sorted_idx = shap_importance.argsort()\n",
    "\n",
    "        # Gráfico de importância SHAP\n",
    "        plt.figure(figsize=(7, 7))\n",
    "        plt.barh(range(len(sorted_idx)), shap_importance[sorted_idx], align='center')\n",
    "        plt.yticks(range(len(sorted_idx)), X_test.columns[sorted_idx])\n",
    "        plt.title('SHAP Importance')\n",
    "        plt.tight_layout()\n",
    "        shap_importance_path = os.path.join(output_dir, \"shap_importance.png\")\n",
    "        plt.savefig(shap_importance_path)\n",
    "        mlflow.log_artifact(shap_importance_path)\n",
    "        plt.close()\n",
    "\n",
    "        # Beeswarm SHAP\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.plots.beeswarm(shap_values, max_display=15, show=False)\n",
    "        plt.title('SHAP Beeswarm')\n",
    "        plt.tight_layout()\n",
    "        shap_beeswarm_path = os.path.join(output_dir, \"shap_beeswarm.png\")\n",
    "        plt.savefig(shap_beeswarm_path)\n",
    "        mlflow.log_artifact(shap_beeswarm_path)\n",
    "        plt.close()\n",
    "\n",
    "        # Registrar o modelo no MLflow\n",
    "        signature = infer_signature(X_test, y_pred_proba)\n",
    "        mlflow.catboost.log_model(\n",
    "            model,\n",
    "            artifact_path=\"model_catboost\",\n",
    "            signature=signature\n",
    "        )\n",
    "\n",
    "        print(metrics)\n",
    "\n",
    "    # Retorna a métrica de perda para o Hyperopt\n",
    "    return {'loss': -metrics['auc'], 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "# Espaço de Busca para o Hyperopt\n",
    "space_catboost = {\n",
    "    'iterations': scope.int(hp.quniform('iterations', 100, 1000, 50)),  # Número de árvores\n",
    "    'learning_rate': hp.loguniform('learning_rate', -3, -0.3),          # Taxa de aprendizado\n",
    "    'depth': scope.int(hp.quniform('depth', 4, 12, 1)),                 # Profundidade das árvores\n",
    "    'l2_leaf_reg': hp.loguniform('l2_leaf_reg', -3, 2),                 # Regularização L2\n",
    "    'random_strength': hp.uniform('random_strength', 0, 2),             # Aleatoriedade nas divisões\n",
    "    'bagging_temperature': hp.uniform('bagging_temperature', 0, 1),     # Temperatura do bagging\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 4, 8),           # Peso para classes desbalanceadas\n",
    "    'min_data_in_leaf': scope.int(hp.quniform('min_data_in_leaf', 10, 100, 10)),  # Mínimo de dados por folha\n",
    "    'max_bin': scope.int(hp.quniform('max_bin', 128, 256, 32)),         # Número máximo de bins\n",
    "    'grow_policy': hp.choice('grow_policy', ['Depthwise', 'Lossguide']),  # Política de crescimento\n",
    "    'eval_metric': 'AUC',\n",
    "    'task_type': 'GPU',                                                 # Utilizar GPU\n",
    "    'random_seed': 42                                                   # Reprodutibilidade\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "187b8c3e-6dd4-495e-8e5e-7d6b4a709b98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                                            | 0/1 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5944134695004141, 'precision': 0.2523631813001408, 'recall_sensibility': 0.7749262334454128, 'f1_score': 0.38073564613465494, 'balanced_accuracy': 0.6673636456561556, 'specificity': 0.5598010578668983, 'auc': 0.7401683259361872, 'prauc': 0.38395847939474775, 'mcc': 0.24599737528156937, 'log_loss': 0.6833430868849364, 'brier_score': 0.24322065228205347, 'cohen_kappa': 0.1822321256436371, 'false_positive_rate_FPR': 0.44019894213310173, 'false_negative_rate_FNR': 0.22507376655458725, 'geometric_mean_GMean': 0.6586383873200476}\n",
      "100%|█████████████████████████████████████████████████| 1/1 [01:10<00:00, 70.65s/trial, best loss: -0.7401683259361872]\n",
      "Melhores hiperparâmetros: {'bagging_temperature': 0.9898837998100424, 'depth': 8.0, 'grow_policy': 0, 'iterations': 750.0, 'l2_leaf_reg': 0.0879471982845018, 'learning_rate': 0.3958231628067325, 'max_bin': 224.0, 'min_data_in_leaf': 90.0, 'random_strength': 1.240237995353282, 'scale_pos_weight': 7.254621630561151}\n"
     ]
    }
   ],
   "source": [
    "# Etapa de hipertunning\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name='Hipertunnig', nested=True,  \n",
    "                      description = 'Busca pelos melhores parametros. Os modelos testados são armazenados, mesmo que não tenha os melhores parametros. CatBoost',\n",
    "                      tags = {\"Execução do Hipert\": \"Melhores parametros\", \"objetivo\": \"garantir os melhores parametros para o modelo\", \"Versão da etapa\": \"1.0\"}):\n",
    "\n",
    "    cat_features = list(X_train_valid.select_dtypes(include=['object']))\n",
    "    # Inicialização do Hyperopt\n",
    "    trials = Trials()\n",
    "    best = fmin(\n",
    "        fn=objective_mlflow,                     # Função objetivo\n",
    "        space=space_catboost,             # Espaço de busca\n",
    "        algo=tpe.suggest,                 # Algoritmo de busca (TPE)\n",
    "        max_evals=1,                     # Número de avaliações\n",
    "        trials=trials,                    # Armazena os resultados\n",
    "        rstate=np.random.default_rng(42)  # Reprodutibilidade\n",
    "    )\n",
    "    \n",
    "   \n",
    "    # Obtendo os melhores hiperparâmetros\n",
    "    mlflow.log_params(best)\n",
    "    print(\"Melhores hiperparâmetros:\", best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081a0cfb-183d-4136-a8de-d868fc8444c6",
   "metadata": {},
   "source": [
    "## Treinamento após hipertunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80a3d320-e1ef-46d5-b685-742316a6dab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107f4f51007e41e794ea00301b32b0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.7008943\tbest: 0.7008943 (0)\ttotal: 28.4ms\tremaining: 21.3s\n",
      "10:\ttest: 0.7340501\tbest: 0.7340501 (10)\ttotal: 278ms\tremaining: 18.7s\n",
      "20:\ttest: 0.7389211\tbest: 0.7389211 (20)\ttotal: 497ms\tremaining: 17.3s\n",
      "30:\ttest: 0.7399414\tbest: 0.7403081 (27)\ttotal: 717ms\tremaining: 16.6s\n",
      "40:\ttest: 0.7391078\tbest: 0.7403275 (32)\ttotal: 970ms\tremaining: 16.8s\n",
      "50:\ttest: 0.7399232\tbest: 0.7403275 (32)\ttotal: 1.19s\tremaining: 16.3s\n",
      "60:\ttest: 0.7395176\tbest: 0.7403275 (32)\ttotal: 1.39s\tremaining: 15.7s\n",
      "70:\ttest: 0.7386327\tbest: 0.7403275 (32)\ttotal: 1.6s\tremaining: 15.3s\n",
      "bestTest = 0.7403274775\n",
      "bestIteration = 32\n",
      "Shrink model to first 33 iterations.\n"
     ]
    }
   ],
   "source": [
    "# Iniciar rastreamento MLflow\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name='Treinamento do melhor modelo modelo CatBoost', nested=True,\n",
    "                     description='Treinando o CatBoost com os melhores parametros',\n",
    "                     tags={\"Versão do modelo\": \"1\", \"Algoritmo\": \"CatBoost\"}):\n",
    "\n",
    "    # Ajuste dos Melhores Parâmetros\n",
    "    best_params = {\n",
    "        'depth': int(best['depth']),\n",
    "        'random_strength': best['random_strength'],\n",
    "        'l2_leaf_reg': best['l2_leaf_reg'],\n",
    "        'bagging_temperature': best['bagging_temperature'],\n",
    "        'min_data_in_leaf': int(best['min_data_in_leaf']),\n",
    "        'learning_rate': best['learning_rate'],\n",
    "        'iterations': int(best['iterations']),\n",
    "        'scale_pos_weight': best['scale_pos_weight'],\n",
    "        'max_bin': int(best['max_bin']),\n",
    "        'grow_policy': ['Depthwise', 'Lossguide'][best['grow_policy']],\n",
    "        'task_type': 'GPU',\n",
    "        'eval_metric': 'AUC',\n",
    "        'loss_function': 'Logloss',\n",
    "        'random_seed': 42,\n",
    "        'verbose': False\n",
    "    }\n",
    "    mlflow.log_params(best_params)\n",
    "    cat_features = list(X_train_valid.select_dtypes(include=['object']))\n",
    "    # Treinamento do Modelo Final\n",
    "    final_model = CatBoostClassifier(**best_params, cat_features=cat_features)\n",
    "\n",
    "    final_model.fit(\n",
    "        X_train_valid, y_train_valid,\n",
    "        eval_set=(X_test_valid, y_test_valid),\n",
    "        early_stopping_rounds=40,\n",
    "        verbose=10,\n",
    "        plot=True\n",
    "    )\n",
    "\n",
    "    # Avaliação do Modelo Final\n",
    "\n",
    "    # Previsões\n",
    "    y_pred_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "    # Métricas de desempenho\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    fpr = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "    fnr = FN / (FN + TP) if (FN + TP) > 0 else 0\n",
    "    g_mean = np.sqrt(sensitivity * specificity)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred),\n",
    "        \"recall_sensitivity\": recall_score(y_test, y_pred),  # Nome ajustado\n",
    "        \"f1_score\": f1_score(y_test, y_pred),\n",
    "        \"balanced_accuracy\": balanced_accuracy_score(y_test, y_pred),\n",
    "        \"specificity\": specificity,\n",
    "        \"auc\": roc_auc_score(y_test, y_pred_proba),\n",
    "        \"prauc\": average_precision_score(y_test, y_pred_proba),\n",
    "        \"mcc\": matthews_corrcoef(y_test, y_pred),\n",
    "        \"log_loss\": log_loss(y_test, y_pred_proba),\n",
    "        \"brier_score\": brier_score_loss(y_test, y_pred_proba),\n",
    "        \"cohen_kappa\": cohen_kappa_score(y_test, y_pred),\n",
    "        \"false_positive_rate_FPR\": fpr,  # Nome ajustado\n",
    "        \"false_negative_rate_FNR\": fnr,  # Nome ajustado\n",
    "        \"geometric_mean_GMean\": g_mean   # Nome ajustado\n",
    "    }\n",
    "\n",
    "    # Log de métricas individualmente\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        mlflow.log_metric(metric_name, metric_value)\n",
    "\n",
    "    # Gráficos e artefatos\n",
    "    # Matriz de Confusão\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\")\n",
    "    plt.title('Matriz de Confusão')\n",
    "    plt.xlabel('Predito')\n",
    "    plt.ylabel('Real')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    mlflow.log_artifact('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Curva ROC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, linestyle='--', label='Curva ROC (AUC = {:.3f})'.format(metrics[\"auc\"]))\n",
    "    plt.title('Curva ROC')\n",
    "    plt.xlabel('Taxa de Falsos Positivos')\n",
    "    plt.ylabel('Taxa de Verdadeiros Positivos')\n",
    "    plt.legend()\n",
    "    plt.savefig('roc_curve.png')\n",
    "    mlflow.log_artifact('roc_curve.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Curva de Precisão-Recall\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall_vals, precision_vals, marker='.', label='PRAUC = {:.3f}'.format(metrics[\"prauc\"]))\n",
    "    plt.title('Curva de Precisão-Recall')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precisão')\n",
    "    plt.legend()\n",
    "    plt.savefig('precision_recall_curve.png')\n",
    "    mlflow.log_artifact('precision_recall_curve.png')\n",
    "    plt.close()\n",
    "\n",
    "    # SHAP Importance\n",
    "    explainer = shap.Explainer(final_model)\n",
    "    shap_values = explainer(X_test)\n",
    "    shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "    sorted_idx = shap_importance.argsort()\n",
    "\n",
    "    # Gráfico de importância SHAP\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.barh(range(len(sorted_idx)), shap_importance[sorted_idx], align='center')\n",
    "    plt.yticks(range(len(sorted_idx)), np.array(X_test.columns)[sorted_idx])\n",
    "    plt.title('SHAP Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('shap_importance.png')\n",
    "    mlflow.log_artifact('shap_importance.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Beeswarm SHAP\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.plots.beeswarm(shap_values, max_display=15, show=False)\n",
    "    plt.title('SHAP Beeswarm')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('shap_beeswarm.png')\n",
    "    mlflow.log_artifact('shap_beeswarm.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Registrar o modelo no MLflow\n",
    "    signature = infer_signature(X_test, y_pred_proba)\n",
    "    mlflow.catboost.log_model(\n",
    "        final_model,\n",
    "        artifact_path=\"model_catboost_final\",\n",
    "        signature=signature\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e9bb3f-4296-43e5-96d3-93bcc3a3f9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
