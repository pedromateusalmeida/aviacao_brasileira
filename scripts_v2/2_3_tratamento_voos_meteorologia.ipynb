{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33bd42d6-4d93-4801-894b-db37e3a4f16e",
   "metadata": {},
   "source": [
    "# Pacote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06c542e1-2c0a-402f-87fc-7ccc874f8937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import chardet\n",
    "import glob\n",
    "from unidecode import unidecode\n",
    "import os\n",
    "import Levenshtein as lev\n",
    "import csv \n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e353b098-1c08-426b-809b-3ef991c7fee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\spark-3.5.0-bin-hadoop3\n",
      "C:\\Users\\pedro\\hadoop3.0\n",
      "C:\\Program Files\\Java\\jdk1.8.0_202\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(os.environ.get(\"SPARK_HOME\"))\n",
    "print(os.environ.get(\"HADOOP_HOME\"))\n",
    "print(os.environ.get(\"JAVA_HOME\"))\n",
    "\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0744d461-401f-4be1-8357-05c779554844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import lower, upper,row_number,isnan, when, count, col, coalesce, broadcast, regexp_replace, regexp_extract, lit, countDistinct\n",
    "from pyspark.sql import functions as F, Window, Row\n",
    "from pyspark.sql.functions import *\n",
    "#from functools import reduce\n",
    "\n",
    "#Pyspark\n",
    "import py4j\n",
    "from pyspark import SparkContext,SQLContext,SparkConf,StorageLevel\n",
    "\n",
    "## Pacotes para configurar sessão no spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "                            \n",
    "## Pacote para localizar o path spark \n",
    "import findspark\n",
    "\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ccaf93-81d0-4b18-85d3-763911ff42dc",
   "metadata": {},
   "source": [
    "## Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f2deba9-4c4c-494d-a69a-170ffdc0c672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usa todos os núcleos disponíveis na máquina local.\n",
    "# Define o nome da aplicação.\n",
    "# Número de núcleos alocados para o driver Spark.\n",
    "# Quantidade de memória alocada para o driver Spark.\n",
    "# Nível de paralelismo padrão para todas as transformações em RDDs.\n",
    "# Número de partições para usar quando fazer operações de shuffle.\n",
    "# Número de instâncias do executor para iniciar.\n",
    "# Número de núcleos para usar por executor.\n",
    "# Quantidade de memória alocada para cada executor.\n",
    "# Fração da heap do executor para armazenamento e execução.\n",
    "# Proporção da memória de execução acima da qual o armazenamento será despejado para o disco.\n",
    "# Habilita o uso de memória fora do heap.\n",
    "# Tamanho da memória fora do heap alocada para o Spark.\n",
    "# Tamanho máximo dos resultados do driver.\n",
    "# Memória adicional alocada por executor.\n",
    "# Habilita a avaliação antecipada e a visualização dos DataFrames no Spark SQL REPL.\n",
    "# Número máximo de linhas para mostrar quando a avaliação antecipada está habilitada.\n",
    "# Tamanho máximo do buffer para serialização Kryo.\n",
    "# Tamanho máximo das tabelas na realização do broadcast join \n",
    "# Usa KryoSerializer para serialização, oferecendo melhor desempenho.\n",
    "# Classe de registrator Kryo para registrar classes personalizadas com Kryo.\n",
    "# Comprime os dados shuffle para economizar espaço em disco.\n",
    "# Define o nível de armazenamento para RDDs persistidos, usando tanto a memória quanto o disco.\n",
    "# Comprime RDDs armazenados em memória.\n",
    "\n",
    "spark = (SparkSession.builder \n",
    "    .master(\"local[*]\") \n",
    "    .appName(\"Spark Optimization\")   \n",
    "    .config(\"spark.driver.cores\", \"2\")   \n",
    "    .config(\"spark.driver.memory\", \"8g\")   \n",
    "    .config(\"spark.default.parallelism\", \"24\")   \n",
    "    .config(\"spark.sql.shuffle.partitions\", \"24\")   \n",
    "    .config(\"spark.executor.instances\", \"3\")   \n",
    "    .config(\"spark.executor.cores\", \"2\")   \n",
    "    .config(\"spark.executor.memory\", \"10g\")   \n",
    "    .config(\"spark.memory.fraction\", \"0.6\")  \n",
    "    .config(\"spark.memory.storageFraction\", \"0.5\")   \n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\")   \n",
    "    .config(\"spark.memory.offHeap.size\", \"4g\")   \n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\")   \n",
    "    .config(\"spark.executor.memoryOverhead\", \"2g\")   \n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)   \n",
    "    .config(\"spark.sql.repl.eagerEval.maxNumRows\", 10)  \n",
    "#    .config(\"spark.kryoserializer.buffer.max\", \"512m\")  \n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"400m\")   \n",
    "#    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")   \n",
    "#    .config(\"spark.kryo.registrator\", \"MyKryoRegistrator\")   \n",
    "    .config(\"spark.shuffle.compress\", \"true\")   \n",
    "    .config(\"spark.storage.level\", \"MEMORY_AND_DISK\")   \n",
    "    .config(\"spark.rdd.compress\", \"true\")   \n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9658301c-ebd6-4bb4-9e68-52d81574d83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-DFIBSP4:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Optimization</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x22d51fbb2d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d85566-5dbe-45c7-98cc-5c22fabe9cbc",
   "metadata": {},
   "source": [
    "# Carregando dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6950e42f-ac85-47dc-a9f7-331066006772",
   "metadata": {},
   "source": [
    "## Df meteorologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb828d95-24a3-48c0-aaef-758856097135",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read.parquet(\"DADOS_METEROLOGIA/DADOS_METEOROLOGICOS_TRATADOS/dados_meteorologicos_2019.parquet\").repartition(12))\n",
    "df = df.withColumn(\"hora_utc\", col(\"hora_utc\").cast(\"int\"))\n",
    "df = df.withColumnRenamed(\"estacao\", \"cidade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e0d1286-9ec0-447b-aba9-2be4f447eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renomeando as colunas para aeroportos de origem\n",
    "df_origem = df.select(\n",
    "    *[col(c).alias(c + '_origem') for c in df.columns])\n",
    "\n",
    "# Renomeando as colunas para aeroportos de destino\n",
    "df_destino = df.select(\n",
    "    *[col(c).alias(c + '_destino') for c in df.columns])\n",
    "\n",
    "df_origem = df_origem.withColumnRenamed(\"data_origem\", \"data_partida\") \\\n",
    "                 .withColumnRenamed(\"hora_utc_origem\", \"hora_partida\")\n",
    "\n",
    "df_destino = df_destino.withColumnRenamed(\"data_destino\", \"data_chegada\")\\\n",
    "                 .withColumnRenamed(\"hora_utc_destino\", \"hora_chegada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da195e8f-be3d-432f-abc6-897a58c2c205",
   "metadata": {},
   "source": [
    "## Df voos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "211d79c8-14df-4d3b-afa2-56b73a973b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_voos=spark.read.option(\"header\", \"true\").csv(\"dados_tratados/historico_voo_tratados_2019.csv\").repartition(6)\n",
    "\n",
    "df_voos = df_voos.withColumn('partida_prevista_data', to_date(col('partida_prevista')))\n",
    "df_voos = df_voos.withColumn('chegada_prevista_data', to_date(col('chegada_prevista')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4d4823b-ea5b-4f29-b883-fce9de7f5d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_voos = df_voos.withColumnRenamed(\"partida_prevista_data\", \"data_partida\") \\\n",
    "                 .withColumnRenamed(\"hora_partida\", \"hora_partida\") \\\n",
    "                 .withColumnRenamed(\"cidade_origem\", \"cidade_origem\") \\\n",
    "                 .withColumnRenamed(\"uf_origem_x\", \"uf_origem\")\n",
    "\n",
    "df_voos = df_voos.withColumnRenamed(\"chegada_prevista_data\", \"data_chegada\") \\\n",
    "                 .withColumnRenamed(\"uf_destino_x\", \"uf_destino\")\n",
    "\n",
    "# Convertendo as colunas data_partida e data_chegada para string\n",
    "df_voos = df_voos.withColumn(\"data_partida\", col(\"data_partida\").cast(\"string\")) \\\n",
    "       .withColumn(\"data_chegada\", col(\"data_chegada\").cast(\"string\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9113ef5-3dbc-4d1a-9cbb-156243dbc707",
   "metadata": {},
   "source": [
    "## Join entre os datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "654985ef-3b98-45bd-aaa2-49115a7ff89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colunas para realizar o join\n",
    "join_cols = ['data_chegada', 'hora_chegada', 'cidade_destino', 'uf_destino']\n",
    "\n",
    "# Realizando o join\n",
    "df_joined = df_voos.join(df_destino, on=join_cols, how='inner').drop(df_destino.data_chegada, df_destino.hora_chegada, df_destino.cidade_destino, df_destino.uf_destino, df_destino.altitude_destino)\n",
    "\n",
    "# Colunas para realizar o join\n",
    "join_cols = ['data_partida','hora_partida','cidade_origem','uf_origem']\n",
    "\n",
    "# Realizando o join\n",
    "df_joined_final = df_joined.join(df_origem, on=join_cols, how='inner').drop(df_origem.data_partida, df_origem.hora_partida, df_origem.cidade_origem, df_origem.uf_origem, df_origem.altitude_origem)\n",
    "\n",
    "df_joined_final = df_joined_final.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03c2112-7851-441c-8573-b47aea5ad3f1",
   "metadata": {},
   "source": [
    "# Salvando o dataset tratado com dados de meterologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccc3badd-03f5-4406-9994-45d818bc6a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reparticionando o DataFrame para uma única partição\n",
    "df_joined_final = df_joined_final.repartition(1)\n",
    "\n",
    "# Salvando o DataFrame em um único arquivo CSV\n",
    "(df_joined_final.write\n",
    "     .option(\"header\", \"true\")\n",
    "     .mode(\"overwrite\")\n",
    "     .csv(\"dados_tratados/historico_voo_meteorologia.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1d0963-7765-473a-a66d-d4f1e8d92553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
